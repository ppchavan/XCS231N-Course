{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fdf2ae1",
      "metadata": {},
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02652c55",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #ffebee; border: 2px solid #d32f2f; padding: 10px; border-radius: 5px; color: #b71c1c;\">\n",
        "        <b>Caution:</b> The setup code below only needs to be run once per assignment. If you've already executed it, you can safely ignore this section and proceed to the <a href=\"#assignment-content\" style=\"color: #d32f2f; text-decoration: underline;\">Assignment Content</a>.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae793c0b",
      "metadata": {},
      "source": [
        "This will run through setup of all necessary dependencies to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda049e1",
      "metadata": {},
      "source": [
        "## Install required tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9bca6094",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # fixing error like \"No module named 'imp'\"\n",
        "    !sed -i -e 's/from imp import reload/from importlib import reload/' /usr/local/lib/python3.12/dist-packages/IPython/extensions/autoreload.py\n",
        "else:\n",
        "  import os\n",
        "  # add local dependencies to PATH (for uv installation)\n",
        "  os.environ['PATH'] = '/usr/local/bin:$HOME/.local/bin:' + os.environ['PATH']\n",
        "  # add conda dependencies to PATH (for conda installation)\n",
        "  os.environ['PATH'] = '/opt/anaconda3:~/anaconda3:~:~/miniconda3:/opt/miniconda3:' + os.environ['PATH']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d5422e",
      "metadata": {},
      "source": [
        "## Install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba057b7",
      "metadata": {},
      "source": [
        "The code snippets below will mount your Google Drive folder (if in a Colab environment) and proceed to install the relevant datasets needed to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "70705ee6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLDERNAME:  d:\\course\\XCS231N-Course\\XCS231N-A3\\src\\submission\n",
            "WORK_DIR:  d:\\course\\XCS231N-Course\\XCS231N-A3\\src\\submission\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    # TODO: For colab, Enter the foldername in your Google Drive\n",
        "    # to the root of the assignment folder, where this notebook exists\n",
        "    # e.g. 'XCS231N/XCS231N-A?/src/submission' # you need to update \"A?\" with the homework number\n",
        "    FOLDERNAME = 'path to your submission folder, see the above description'\n",
        "    WORK_DIR = '/content/drive/MyDrive/{}'.format(FOLDERNAME)\n",
        "    # This mounts your Google Drive to the Colab VM.\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "    # Now that we've mounted your Drive, this ensures that\n",
        "    # the Python interpreter of the Colab VM can load\n",
        "    # python files from within it.\n",
        "    import sys\n",
        "    sys.path.append(WORK_DIR)\n",
        "else:\n",
        "    # on local or azure, FOLDERNAME is set automatically\n",
        "    FOLDERNAME=os.getcwd()\n",
        "    WORK_DIR = FOLDERNAME\n",
        "\n",
        "print(f\"FOLDERNAME: \", FOLDERNAME)\n",
        "print(f\"WORK_DIR: \", WORK_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f537ac6b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting dataset: START\n",
            "d:\\course\\XCS231N-Course\\XCS231N-A3\\src\\submission\\xcs231n\\datasets\n",
            "wget not found, using curl to download datasets...\n",
            "Archive:  coco_captioning.zip\n",
            "   creating: coco_captioning/\n",
            "  inflating: coco_captioning/coco2014_captions.h5  \n",
            "  inflating: coco_captioning/coco2014_vocab.json  \n",
            "  inflating: coco_captioning/train2014_images.txt  \n",
            "  inflating: coco_captioning/train2014_urls.txt  \n",
            "  inflating: coco_captioning/train2014_vgg16_fc7.h5  \n",
            "  inflating: coco_captioning/train2014_vgg16_fc7_pca.h5  \n",
            "  inflating: coco_captioning/val2014_images.txt  \n",
            "  inflating: coco_captioning/val2014_urls.txt  \n",
            "  inflating: coco_captioning/val2014_vgg16_fc7.h5  \n",
            "  inflating: coco_captioning/val2014_vgg16_fc7_pca.h5  \n",
            "d:\\course\\XCS231N-Course\\XCS231N-A3\\src\\submission\n",
            "Getting dataset: DONE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   255  100   255    0     0    748      0 --:--:-- --:--:-- --:--:--   806\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0 3848k    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "  7 3848k    7  288k    0     0  69785      0  0:00:56  0:00:04  0:00:52  200k\n",
            " 27 3848k   27 1056k    0     0   197k      0  0:00:19  0:00:05  0:00:14  413k\n",
            " 34 3848k   34 1344k    0     0   214k      0  0:00:17  0:00:06  0:00:11  386k\n",
            " 48 3848k   48 1872k    0     0   257k      0  0:00:14  0:00:07  0:00:07  417k\n",
            " 59 3848k   59 2272k    0     0   275k      0  0:00:13  0:00:08  0:00:05  456k\n",
            " 69 3848k   69 2672k    0     0   287k      0  0:00:13  0:00:09  0:00:04  470k\n",
            " 79 3848k   79 3056k    0     0   298k      0  0:00:12  0:00:10  0:00:02  409k\n",
            " 93 3848k   93 3584k    0     0   317k      0  0:00:12  0:00:11  0:00:01  445k\n",
            "100 3848k  100 3848k    0     0   330k      0  0:00:11  0:00:11 --:--:--  450k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   255  100   255    0     0    380      0 --:--:-- --:--:-- --:--:--   381\n",
            "\n",
            "  0  987M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0  987M    0  160k    0     0  65420      0  4:23:44  0:00:02  4:23:42  175k\n",
            "  0  987M    0  304k    0     0  86535      0  3:19:22  0:00:03  3:19:19  151k\n",
            "  0  987M    0  368k    0     0  82467      0  3:29:13  0:00:04  3:29:09  123k\n",
            "  0  987M    0  464k    0     0  85734      0  3:21:14  0:00:05  3:21:09  117k\n",
            "  0  987M    0  528k    0     0  83011      0  3:27:50  0:00:06  3:27:44  107k\n",
            "  0  987M    0  608k    0     0  82850      0  3:28:14  0:00:07  3:28:07 91567\n",
            "  0  987M    0  720k    0     0  86265      0  3:20:00  0:00:08  3:19:52 86057\n",
            "  0  987M    0  848k    0     0  91493      0  3:08:34  0:00:09  3:08:25 99861\n",
            "  0  987M    0  992k    0     0  96808      0  2:58:13  0:00:10  2:58:03  106k\n",
            "  0  987M    0 1216k    0     0   105k      0  2:39:25  0:00:11  2:39:14  137k\n",
            "  0  987M    0 1536k    0     0   122k      0  2:17:35  0:00:12  2:17:23  184k\n",
            "  0  987M    0 1920k    0     0   141k      0  1:58:40  0:00:13  1:58:27  241k\n",
            "  0  987M    0 2560k    0     0   176k      0  1:35:36  0:00:14  1:35:22  340k\n",
            "  0  987M    0 2992k    0     0   192k      0  1:27:22  0:00:15  1:27:07  398k\n",
            "  0  987M    0 3328k    0     0   201k      0  1:23:41  0:00:16  1:23:25  420k\n",
            "  0  987M    0 3936k    0     0   224k      0  1:15:02  0:00:17  1:14:45  481k\n",
            "  0  987M    0 4400k    0     0   237k      0  1:10:57  0:00:18  1:10:39  495k\n",
            "  0  987M    0 4624k    0     0   236k      0  1:11:08  0:00:19  1:10:49  412k\n",
            "  0  987M    0 4832k    0     0   234k      0  1:11:49  0:00:20  1:11:29  362k\n",
            "  0  987M    0 5104k    0     0   236k      0  1:11:09  0:00:21  1:10:48  353k\n",
            "  0  987M    0 5616k    0     0   249k      0  1:07:36  0:00:22  1:07:14  335k\n",
            "  0  987M    0 5984k    0     0   254k      0  1:06:10  0:00:23  1:05:47  318k\n",
            "  0  987M    0 6272k    0     0   254k      0  1:06:04  0:00:24  1:05:40  324k\n",
            "  0  987M    0 6448k    0     0   250k      0  1:07:08  0:00:25  1:06:43  317k\n",
            "  0  987M    0 6656k    0     0   249k      0  1:07:38  0:00:26  1:07:12  300k\n",
            "  0  987M    0 6880k    0     0   249k      0  1:07:34  0:00:27  1:07:07  249k\n",
            "  0  987M    0 7120k    0     0   249k      0  1:07:39  0:00:28  1:07:11  223k\n",
            "  0  987M    0 7392k    0     0   250k      0  1:07:16  0:00:29  1:06:47  227k\n",
            "  0  987M    0 7792k    0     0   251k      0  1:06:58  0:00:30  1:06:28  254k\n",
            "  0  987M    0 7968k    0     0   252k      0  1:06:43  0:00:31  1:06:12  271k\n",
            "  0  987M    0 8368k    0     0   257k      0  1:05:28  0:00:32  1:04:56  302k\n",
            "  0  987M    0 8800k    0     0   262k      0  1:04:07  0:00:33  1:03:34  342k\n",
            "  0  987M    0 9152k    0     0   265k      0  1:03:31  0:00:34  1:02:57  352k\n",
            "  0  987M    0 9456k    0     0   266k      0  1:03:14  0:00:35  1:02:39  367k\n",
            "  0  987M    0 9712k    0     0   265k      0  1:03:22  0:00:36  1:02:46  350k\n",
            "  0  987M    0 9856k    0     0   262k      0  1:04:14  0:00:37  1:03:37  293k\n",
            "  0  987M    0  9.8M    0     0   260k      0  1:04:43  0:00:38  1:04:05  244k\n",
            "  1  987M    1  9.9M    0     0   258k      0  1:05:15  0:00:39  1:04:36  209k\n",
            "  1  987M    1 10.0M    0     0   254k      0  1:06:11  0:00:40  1:05:31  172k\n",
            "  1  987M    1 10.2M    0     0   251k      0  1:06:55  0:00:41  1:06:14  149k\n",
            "  1  987M    1 10.3M    0     0   249k      0  1:07:25  0:00:42  1:06:43  157k\n",
            "  1  987M    1 10.5M    0     0   248k      0  1:07:46  0:00:43  1:07:03  157k\n",
            "  1  987M    1 10.7M    0     0   246k      0  1:08:28  0:00:44  1:07:44  153k\n",
            "  1  987M    1 10.8M    0     0   244k      0  1:08:57  0:00:45  1:08:12  160k\n",
            "  1  987M    1 11.0M    0     0   242k      0  1:09:36  0:00:46  1:08:50  161k\n",
            "  1  987M    1 11.2M    0     0   242k      0  1:09:34  0:00:47  1:08:47  175k\n",
            "  1  987M    1 11.4M    0     0   242k      0  1:09:31  0:00:48  1:08:43  187k\n",
            "  1  987M    1 11.8M    0     0   244k      0  1:08:58  0:00:49  1:08:09  227k\n",
            "  1  987M    1 12.2M    0     0   247k      0  1:08:08  0:00:50  1:07:18  273k\n",
            "  1  987M    1 12.5M    0     0   249k      0  1:07:33  0:00:51  1:06:42  317k\n",
            "  1  987M    1 12.7M    0     0   247k      0  1:07:58  0:00:52  1:07:06  301k\n",
            "  1  987M    1 12.9M    0     0   246k      0  1:08:17  0:00:53  1:07:24  289k\n",
            "  1  987M    1 13.1M    0     0   246k      0  1:08:28  0:00:54  1:07:34  263k\n",
            "  1  987M    1 13.2M    0     0   243k      0  1:09:04  0:00:55  1:08:09  210k\n",
            "  1  987M    1 13.2M    0     0   240k      0  1:10:04  0:00:56  1:09:08  147k\n",
            "  1  987M    1 13.2M    0     0   233k      0  1:12:07  0:00:58  1:11:09  101k\n",
            "  1  987M    1 13.2M    0     0   230k      0  1:13:02  0:00:59  1:12:03 74908\n",
            "  1  987M    1 13.3M    0     0   228k      0  1:13:37  0:00:59  1:12:38 42513\n",
            "  1  987M    1 13.3M    0     0   226k      0  1:14:31  0:01:00  1:13:31 29332\n",
            "  1  987M    1 13.4M    0     0   223k      0  1:15:26  0:01:01  1:14:25 35220\n",
            "  1  987M    1 13.6M    0     0   222k      0  1:15:46  0:01:02  1:14:44 79569\n",
            "  1  987M    1 13.7M    0     0   221k      0  1:15:56  0:01:03  1:14:53  106k\n",
            "  1  987M    1 14.0M    0     0   222k      0  1:15:42  0:01:04  1:14:38  150k\n",
            "  1  987M    1 14.2M    0     0   222k      0  1:15:48  0:01:05  1:14:43  175k\n",
            "  1  987M    1 14.3M    0     0   220k      0  1:16:17  0:01:06  1:15:11  189k\n",
            "  1  987M    1 14.4M    0     0   217k      0  1:17:20  0:01:08  1:16:12  164k\n",
            "  1  987M    1 14.5M    0     0   217k      0  1:17:31  0:01:08  1:16:23  160k\n",
            "  1  987M    1 14.9M    0     0   220k      0  1:16:33  0:01:09  1:15:24  186k\n",
            "  1  987M    1 15.1M    0     0   219k      0  1:16:38  0:01:10  1:15:28  188k\n",
            "  1  987M    1 15.4M    0     0   221k      0  1:16:13  0:01:11  1:15:02  223k\n",
            "  1  987M    1 15.7M    0     0   222k      0  1:15:45  0:01:12  1:14:33  289k\n",
            "  1  987M    1 16.0M    0     0   223k      0  1:15:21  0:01:13  1:14:08  308k\n",
            "  1  987M    1 16.2M    0     0   223k      0  1:15:25  0:01:14  1:14:11  268k\n",
            "  1  987M    1 16.5M    0     0   223k      0  1:15:18  0:01:15  1:14:03  278k\n",
            "  1  987M    1 16.6M    0     0   223k      0  1:15:26  0:01:16  1:14:10  256k\n",
            "  1  987M    1 16.9M    0     0   222k      0  1:15:34  0:01:17  1:14:17  230k\n",
            "  1  987M    1 17.2M    0     0   225k      0  1:14:40  0:01:18  1:13:22  255k\n",
            "  1  987M    1 17.5M    0     0   225k      0  1:14:38  0:01:19  1:13:19  261k\n",
            "  1  987M    1 17.7M    0     0   226k      0  1:14:30  0:01:20  1:13:10  262k\n",
            "  1  987M    1 18.0M    0     0   226k      0  1:14:15  0:01:21  1:12:54  280k\n",
            "  1  987M    1 18.4M    0     0   228k      0  1:13:45  0:01:22  1:12:23  315k\n",
            "  1  987M    1 18.6M    0     0   228k      0  1:13:39  0:01:23  1:12:16  277k\n",
            "  1  987M    1 18.8M    0     0   228k      0  1:13:45  0:01:24  1:12:21  270k\n",
            "  1  987M    1 19.1M    0     0   229k      0  1:13:31  0:01:25  1:12:06  277k\n",
            "  1  987M    1 19.3M    0     0   229k      0  1:13:34  0:01:26  1:12:08  262k\n",
            "  1  987M    1 19.5M    0     0   228k      0  1:13:34  0:01:27  1:12:07  238k\n",
            "  2  987M    2 19.8M    0     0   229k      0  1:13:16  0:01:28  1:11:48  250k\n",
            "  2  987M    2 20.0M    0     0   228k      0  1:13:40  0:01:29  1:12:11  233k\n",
            "  2  987M    2 20.1M    0     0   228k      0  1:13:46  0:01:30  1:12:16  214k\n",
            "  2  987M    2 20.4M    0     0   228k      0  1:13:39  0:01:31  1:12:08  223k\n",
            "  2  987M    2 20.6M    0     0   228k      0  1:13:51  0:01:32  1:12:19  214k\n",
            "  2  987M    2 20.8M    0     0   227k      0  1:13:56  0:01:33  1:12:23  190k\n",
            "  2  987M    2 20.9M    0     0   227k      0  1:14:12  0:01:34  1:12:38  197k\n",
            "  2  987M    2 21.0M    0     0   225k      0  1:14:36  0:01:35  1:13:01  179k\n",
            "  2  987M    2 21.1M    0     0   224k      0  1:14:56  0:01:36  1:13:20  153k\n",
            "  2  987M    2 21.3M    0     0   223k      0  1:15:15  0:01:37  1:13:38  142k\n",
            "  2  987M    2 21.4M    0     0   223k      0  1:15:26  0:01:38  1:13:48  138k\n",
            "  2  987M    2 21.6M    0     0   222k      0  1:15:38  0:01:39  1:13:59  139k\n",
            "  2  987M    2 21.7M    0     0   221k      0  1:16:09  0:01:40  1:14:29  133k\n",
            "  2  987M    2 21.7M    0     0   219k      0  1:16:36  0:01:41  1:14:55  124k\n",
            "  2  987M    2 21.8M    0     0   218k      0  1:17:08  0:01:42  1:15:26  111k\n",
            "  2  987M    2 22.0M    0     0   217k      0  1:17:26  0:01:43  1:15:43  109k\n",
            "  2  987M    2 22.3M    0     0   218k      0  1:17:03  0:01:44  1:15:19  138k\n",
            "  2  987M    2 22.6M    0     0   219k      0  1:16:40  0:01:45  1:14:55  189k\n",
            "  2  987M    2 22.8M    0     0   219k      0  1:16:39  0:01:46  1:14:53  217k\n",
            "  2  987M    2 23.1M    0     0   220k      0  1:16:34  0:01:47  1:14:47  252k\n",
            "  2  987M    2 23.4M    0     0   221k      0  1:16:09  0:01:48  1:14:21  300k\n",
            "  2  987M    2 23.8M    0     0   222k      0  1:15:42  0:01:49  1:13:53  304k\n",
            "  2  987M    2 24.1M    0     0   223k      0  1:15:21  0:01:50  1:13:31  303k\n",
            "  2  987M    2 24.4M    0     0   224k      0  1:14:56  0:01:51  1:13:05  331k\n",
            "  2  987M    2 24.8M    0     0   226k      0  1:14:32  0:01:52  1:12:40  356k\n",
            "  2  987M    2 25.1M    0     0   227k      0  1:14:10  0:01:53  1:12:17  355k\n",
            "  2  987M    2 25.6M    0     0   229k      0  1:13:34  0:01:54  1:11:40  371k\n",
            "  2  987M    2 26.0M    0     0   230k      0  1:13:06  0:01:55  1:11:11  383k\n",
            "  2  987M    2 26.5M    0     0   233k      0  1:12:17  0:01:56  1:10:21  416k\n",
            "  2  987M    2 26.9M    0     0   234k      0  1:11:42  0:01:57  1:09:45  436k\n",
            "  2  987M    2 27.3M    0     0   236k      0  1:11:17  0:01:58  1:09:19  444k\n",
            "  2  987M    2 27.8M    0     0   238k      0  1:10:37  0:01:59  1:08:38  455k\n",
            "  2  987M    2 28.3M    0     0   240k      0  1:09:58  0:02:00  1:07:58  479k\n",
            "  2  987M    2 28.8M    0     0   243k      0  1:09:18  0:02:01  1:07:17  475k\n",
            "  2  987M    2 29.3M    0     0   245k      0  1:08:45  0:02:02  1:06:43  483k\n",
            "  3  987M    3 29.7M    0     0   246k      0  1:08:20  0:02:03  1:06:17  486k\n",
            "  3  987M    3 30.1M    0     0   247k      0  1:08:04  0:02:04  1:06:00  457k\n",
            "  3  987M    3 30.3M    0     0   247k      0  1:07:57  0:02:05  1:05:52  419k\n",
            "  3  987M    3 30.6M    0     0   248k      0  1:07:52  0:02:06  1:05:46  373k\n",
            "  3  987M    3 30.8M    0     0   248k      0  1:07:55  0:02:07  1:05:48  322k\n",
            "  3  987M    3 31.0M    0     0   247k      0  1:08:12  0:02:08  1:06:04  259k\n",
            "  3  987M    3 31.1M    0     0   246k      0  1:08:25  0:02:09  1:06:16  214k\n",
            "  3  987M    3 31.3M    0     0   246k      0  1:08:28  0:02:10  1:06:18  199k\n",
            "  3  987M    3 31.5M    0     0   245k      0  1:08:36  0:02:11  1:06:25  180k\n",
            "  3  987M    3 31.7M    0     0   245k      0  1:08:41  0:02:12  1:06:29  173k\n",
            "  3  987M    3 31.8M    0     0   244k      0  1:08:55  0:02:13  1:06:42  178k\n",
            "  3  987M    3 32.0M    0     0   243k      0  1:09:09  0:02:14  1:06:55  175k\n",
            "  3  987M    3 32.3M    0     0   244k      0  1:09:01  0:02:15  1:06:46  194k\n",
            "  3  987M    3 32.5M    0     0   244k      0  1:09:00  0:02:16  1:06:44  206k\n",
            "  3  987M    3 32.8M    0     0   244k      0  1:08:50  0:02:17  1:06:33  230k\n",
            "  3  987M    3 33.0M    0     0   244k      0  1:08:52  0:02:18  1:06:34  249k\n",
            "  3  987M    3 33.3M    0     0   244k      0  1:08:52  0:02:19  1:06:33  271k\n",
            "  3  987M    3 33.5M    0     0   244k      0  1:08:52  0:02:20  1:06:32  258k\n",
            "  3  987M    3 33.9M    0     0   245k      0  1:08:36  0:02:21  1:06:15  284k\n",
            "  3  987M    3 34.3M    0     0   246k      0  1:08:18  0:02:22  1:05:56  297k\n",
            "  3  987M    3 34.6M    0     0   247k      0  1:08:05  0:02:23  1:05:42  326k\n",
            "  3  987M    3 35.1M    0     0   249k      0  1:07:38  0:02:24  1:05:14  372k\n",
            "  3  987M    3 35.6M    0     0   250k      0  1:07:11  0:02:25  1:04:46  425k\n",
            "  3  987M    3 36.0M    0     0   251k      0  1:06:59  0:02:26  1:04:33  417k\n",
            "  3  987M    3 36.3M    0     0   252k      0  1:06:49  0:02:27  1:04:22  410k\n",
            "  3  987M    3 36.6M    0     0   252k      0  1:06:40  0:02:28  1:04:12  403k\n",
            "  3  987M    3 37.0M    0     0   254k      0  1:06:20  0:02:29  1:03:51  396k\n",
            "  3  987M    3 37.5M    0     0   255k      0  1:05:52  0:02:30  1:03:22  402k\n",
            "  3  987M    3 38.2M    0     0   258k      0  1:05:13  0:02:31  1:02:42  459k\n",
            "  3  987M    3 38.9M    0     0   261k      0  1:04:23  0:02:32  1:01:51  545k\n",
            "  3  987M    3 39.4M    0     0   263k      0  1:04:03  0:02:33  1:01:30  571k\n",
            "  4  987M    4 39.8M    0     0   264k      0  1:03:46  0:02:34  1:01:12  568k\n",
            "  4  987M    4 40.7M    0     0   268k      0  1:02:51  0:02:35  1:00:16  634k\n",
            "  4  987M    4 41.9M    0     0   274k      0  1:01:25  0:02:36  0:58:49  761k\n",
            "  4  987M    4 43.0M    0     0   279k      0  1:00:12  0:02:37  0:57:35  836k\n",
            "  4  987M    4 43.5M    0     0   281k      0  0:59:52  0:02:38  0:57:14  841k\n",
            "  4  987M    4 44.1M    0     0   283k      0  0:59:25  0:02:39  0:56:46  885k\n",
            "  4  987M    4 44.5M    0     0   284k      0  0:59:14  0:02:40  0:56:34  794k\n",
            "  4  987M    4 44.8M    0     0   284k      0  0:59:13  0:02:41  0:56:32  603k\n",
            "  4  987M    4 45.0M    0     0   283k      0  0:59:22  0:02:42  0:56:40  405k\n",
            "  4  987M    4 45.2M    0     0   283k      0  0:59:27  0:02:43  0:56:44  347k\n",
            "  4  987M    4 45.4M    0     0   282k      0  0:59:34  0:02:44  0:56:50  260k\n",
            "  4  987M    4 45.7M    0     0   282k      0  0:59:34  0:02:45  0:56:49  230k\n",
            "  4  987M    4 46.1M    0     0   283k      0  0:59:21  0:02:46  0:56:35  261k\n",
            "  4  987M    4 46.4M    0     0   283k      0  0:59:27  0:02:47  0:56:40  270k\n",
            "  4  987M    4 46.5M    0     0   282k      0  0:59:36  0:02:48  0:56:48  259k\n",
            "  4  987M    4 46.6M    0     0   281k      0  0:59:47  0:02:49  0:56:58  246k\n",
            "  4  987M    4 46.8M    0     0   281k      0  0:59:54  0:02:50  0:57:04  229k\n",
            "  4  987M    4 47.0M    0     0   280k      0  1:00:01  0:02:51  0:57:10  177k\n",
            "  4  987M    4 47.2M    0     0   280k      0  1:00:02  0:02:52  0:57:10  184k\n",
            "  4  987M    4 47.5M    0     0   280k      0  1:00:05  0:02:53  0:57:12  204k\n",
            "  4  987M    4 47.6M    0     0   279k      0  1:00:15  0:02:54  0:57:21  206k\n",
            "  4  987M    4 47.8M    0     0   279k      0  1:00:22  0:02:55  0:57:27  205k\n",
            "  4  987M    4 48.0M    0     0   278k      0  1:00:32  0:02:56  0:57:36  198k\n",
            "  4  987M    4 48.2M    0     0   278k      0  1:00:35  0:02:57  0:57:38  190k\n",
            "  4  987M    4 48.4M    0     0   277k      0  1:00:40  0:02:58  0:57:42  185k\n",
            "  4  987M    4 48.6M    0     0   277k      0  1:00:39  0:02:59  0:57:40  211k\n",
            "  4  987M    4 48.9M    0     0   277k      0  1:00:41  0:03:00  0:57:41  229k\n",
            "  4  987M    4 49.1M    0     0   277k      0  1:00:49  0:03:01  0:57:48  230k\n",
            "  4  987M    4 49.2M    0     0   276k      0  1:00:57  0:03:02  0:57:55  216k\n",
            "  5  987M    5 49.4M    0     0   275k      0  1:01:05  0:03:03  0:58:02  209k\n",
            "  5  987M    5 49.6M    0     0   275k      0  1:01:08  0:03:04  0:58:04  198k\n",
            "  5  987M    5 49.9M    0     0   275k      0  1:01:11  0:03:05  0:58:06  193k\n",
            "  5  987M    5 50.1M    0     0   275k      0  1:01:12  0:03:06  0:58:06  211k\n",
            "  5  987M    5 50.4M    0     0   275k      0  1:01:13  0:03:07  0:58:06  232k\n",
            "  5  987M    5 50.6M    0     0   274k      0  1:01:16  0:03:08  0:58:08  243k\n",
            "  5  987M    5 50.9M    0     0   274k      0  1:01:17  0:03:09  0:58:08  252k\n",
            "  5  987M    5 51.1M    0     0   274k      0  1:01:18  0:03:10  0:58:08  254k\n",
            "  5  987M    5 51.5M    0     0   275k      0  1:01:09  0:03:11  0:57:58  284k\n",
            "  5  987M    5 51.9M    0     0   276k      0  1:01:01  0:03:12  0:57:49  309k\n",
            "  5  987M    5 52.3M    0     0   277k      0  1:00:48  0:03:13  0:57:35  354k\n",
            "  5  987M    5 52.8M    0     0   278k      0  1:00:35  0:03:14  0:57:21  399k\n",
            "  5  987M    5 53.2M    0     0   279k      0  1:00:23  0:03:15  0:57:08  439k\n",
            "  5  987M    5 54.3M    0     0   282k      0  0:59:32  0:03:16  0:56:16  561k\n",
            "  5  987M    5 54.7M    0     0   284k      0  0:59:19  0:03:17  0:56:02  590k\n",
            "  5  987M    5 55.4M    0     0   285k      0  0:58:56  0:03:18  0:55:38  615k\n",
            "  5  987M    5 55.9M    0     0   287k      0  0:58:39  0:03:19  0:55:20  648k\n",
            "  5  987M    5 56.5M    0     0   288k      0  0:58:21  0:03:20  0:55:01  668k\n",
            "  5  987M    5 57.0M    0     0   289k      0  0:58:07  0:03:21  0:54:46  567k\n",
            "  5  987M    5 57.5M    0     0   291k      0  0:57:53  0:03:22  0:54:31  566k\n",
            "  5  987M    5 58.0M    0     0   291k      0  0:57:43  0:03:23  0:54:20  541k\n",
            "  5  987M    5 58.7M    0     0   293k      0  0:57:19  0:03:24  0:53:55  556k\n",
            "  6  987M    6 59.2M    0     0   295k      0  0:57:06  0:03:25  0:53:41  540k\n",
            "  6  987M    6 59.6M    0     0   295k      0  0:57:00  0:03:26  0:53:34  523k\n",
            "  6  987M    6 59.9M    0     0   295k      0  0:56:58  0:03:27  0:53:31  486k\n",
            "  6  987M    6 60.1M    0     0   295k      0  0:57:02  0:03:28  0:53:34  437k\n",
            "  6  987M    6 60.5M    0     0   295k      0  0:56:59  0:03:29  0:53:30  368k\n",
            "  6  987M    6 60.9M    0     0   296k      0  0:56:50  0:03:30  0:53:20  351k\n",
            "  6  987M    6 61.3M    0     0   297k      0  0:56:43  0:03:31  0:53:12  361k\n",
            "  6  987M    6 61.7M    0     0   297k      0  0:56:38  0:03:32  0:53:06  370k\n",
            "  6  987M    6 62.2M    0     0   298k      0  0:56:24  0:03:33  0:52:51  435k\n",
            "  6  987M    6 62.7M    0     0   299k      0  0:56:15  0:03:34  0:52:41  457k\n",
            "  6  987M    6 63.2M    0     0   300k      0  0:56:05  0:03:35  0:52:30  468k\n",
            "  6  987M    6 63.4M    0     0   300k      0  0:56:07  0:03:36  0:52:31  432k\n",
            "  6  987M    6 63.6M    0     0   299k      0  0:56:13  0:03:37  0:52:36  395k\n",
            "  6  987M    6 63.8M    0     0   299k      0  0:56:16  0:03:38  0:52:38  331k\n",
            "  6  987M    6 64.0M    0     0   298k      0  0:56:21  0:03:39  0:52:42  276k\n",
            "  6  987M    6 64.2M    0     0   298k      0  0:56:27  0:03:40  0:52:47  217k\n",
            "  6  987M    6 64.5M    0     0   298k      0  0:56:29  0:03:41  0:52:48  215k\n",
            "  6  987M    6 64.7M    0     0   297k      0  0:56:34  0:03:42  0:52:52  214k\n",
            "  6  987M    6 65.0M    0     0   297k      0  0:56:33  0:03:43  0:52:50  232k\n",
            "  6  987M    6 65.4M    0     0   298k      0  0:56:26  0:03:44  0:52:42  281k\n",
            "  6  987M    6 65.7M    0     0   298k      0  0:56:28  0:03:45  0:52:43  294k\n",
            "  6  987M    6 65.9M    0     0   297k      0  0:56:33  0:03:46  0:52:47  283k\n",
            "  6  987M    6 66.0M    0     0   297k      0  0:56:42  0:03:47  0:52:55  267k\n",
            "  6  987M    6 66.1M    0     0   296k      0  0:56:48  0:03:48  0:53:00  235k\n",
            "  6  987M    6 66.4M    0     0   296k      0  0:56:52  0:03:49  0:53:03  194k\n",
            "  6  987M    6 66.7M    0     0   296k      0  0:56:50  0:03:50  0:53:00  203k\n",
            "  6  987M    6 66.9M    0     0   295k      0  0:56:55  0:03:51  0:53:04  207k\n",
            "  6  987M    6 67.0M    0     0   295k      0  0:57:03  0:03:52  0:53:11  208k\n",
            "  6  987M    6 67.1M    0     0   294k      0  0:57:14  0:03:53  0:53:21  192k\n",
            "  6  987M    6 67.2M    0     0   293k      0  0:57:20  0:03:54  0:53:26  180k\n",
            "  6  987M    6 67.5M    0     0   293k      0  0:57:28  0:03:55  0:53:33  150k\n",
            "  6  987M    6 67.5M    0     0   292k      0  0:57:36  0:03:56  0:53:40  132k\n",
            "  6  987M    6 67.6M    0     0   291k      0  0:57:44  0:03:57  0:53:47  131k\n",
            "  6  987M    6 67.9M    0     0   291k      0  0:57:48  0:03:58  0:53:50  156k\n",
            "  6  987M    6 68.1M    0     0   291k      0  0:57:51  0:03:59  0:53:52  170k\n",
            "  6  987M    6 68.3M    0     0   291k      0  0:57:51  0:04:00  0:53:51  194k\n",
            "  6  987M    6 68.6M    0     0   291k      0  0:57:53  0:04:01  0:53:52  220k\n",
            "  6  987M    6 68.8M    0     0   290k      0  0:57:59  0:04:02  0:53:57  229k\n",
            "  6  987M    6 68.9M    0     0   290k      0  0:58:05  0:04:03  0:54:02  223k\n",
            "  7  987M    7 69.1M    0     0   289k      0  0:58:11  0:04:04  0:54:07  211k\n",
            "  7  987M    7 69.4M    0     0   289k      0  0:58:13  0:04:05  0:54:08  205k\n",
            "  7  987M    7 69.6M    0     0   289k      0  0:58:16  0:04:06  0:54:10  198k\n",
            "  7  987M    7 69.7M    0     0   288k      0  0:58:23  0:04:07  0:54:16  193k\n",
            "  7  987M    7 69.9M    0     0   288k      0  0:58:26  0:04:08  0:54:18  202k\n",
            "  7  987M    7 70.0M    0     0   287k      0  0:58:34  0:04:09  0:54:25  193k\n",
            "  7  987M    7 70.2M    0     0   287k      0  0:58:41  0:04:10  0:54:31  172k\n",
            "  7  987M    7 70.4M    0     0   286k      0  0:58:45  0:04:11  0:54:34  169k\n",
            "  7  987M    7 70.6M    0     0   286k      0  0:58:50  0:04:12  0:54:38  177k\n",
            "  7  987M    7 70.8M    0     0   285k      0  0:58:55  0:04:13  0:54:42  171k\n",
            "  7  987M    7 71.0M    0     0   285k      0  0:58:56  0:04:14  0:54:42  194k\n",
            "  7  987M    7 71.4M    0     0   286k      0  0:58:48  0:04:15  0:54:33  255k\n",
            "  7  987M    7 71.7M    0     0   286k      0  0:58:47  0:04:16  0:54:31  275k\n",
            "  7  987M    7 72.1M    0     0   286k      0  0:58:45  0:04:17  0:54:28  309k\n",
            "  7  987M    7 72.6M    0     0   287k      0  0:58:35  0:04:18  0:54:17  370k\n",
            "  7  987M    7 73.2M    0     0   288k      0  0:58:18  0:04:19  0:53:59  451k\n",
            "  7  987M    7 73.6M    0     0   289k      0  0:58:12  0:04:20  0:53:52  438k\n",
            "  7  987M    7 74.0M    0     0   289k      0  0:58:07  0:04:21  0:53:46  463k\n",
            "  7  987M    7 74.2M    0     0   289k      0  0:58:10  0:04:22  0:53:48  439k\n",
            "  7  987M    7 74.5M    0     0   289k      0  0:58:08  0:04:23  0:53:45  403k\n",
            "  7  987M    7 75.0M    0     0   290k      0  0:58:01  0:04:24  0:53:37  360k\n",
            "  7  987M    7 75.4M    0     0   290k      0  0:57:55  0:04:25  0:53:30  364k\n",
            "  7  987M    7 75.7M    0     0   290k      0  0:57:55  0:04:26  0:53:29  342k\n",
            "  7  987M    7 76.1M    0     0   291k      0  0:57:49  0:04:27  0:53:22  384k\n",
            "  7  987M    7 76.5M    0     0   291k      0  0:57:45  0:04:28  0:53:17  396k\n",
            "  7  987M    7 76.7M    0     0   291k      0  0:57:47  0:04:29  0:53:18  355k\n",
            "  7  987M    7 77.0M    0     0   291k      0  0:57:46  0:04:30  0:53:16  332k\n",
            "  7  987M    7 77.5M    0     0   292k      0  0:57:39  0:04:31  0:53:08  362k\n",
            "  7  987M    7 77.8M    0     0   292k      0  0:57:36  0:04:32  0:53:04  349k\n",
            "  7  987M    7 78.4M    0     0   293k      0  0:57:27  0:04:33  0:52:54  367k\n",
            "  7  987M    7 78.6M    0     0   293k      0  0:57:28  0:04:34  0:52:54  379k\n",
            "  7  987M    7 78.8M    0     0   293k      0  0:57:29  0:04:35  0:52:54  369k\n",
            "  8  987M    8 79.0M    0     0   292k      0  0:57:33  0:04:36  0:52:57  318k\n",
            "  8  987M    8 79.4M    0     0   293k      0  0:57:29  0:04:37  0:52:52  324k\n",
            "  8  987M    8 79.6M    0     0   292k      0  0:57:30  0:04:38  0:52:52  276k\n",
            "  8  987M    8 79.9M    0     0   292k      0  0:57:34  0:04:39  0:52:55  264k\n",
            "  8  987M    8 80.0M    0     0   292k      0  0:57:39  0:04:40  0:52:59  247k\n",
            "  8  987M    8 80.2M    0     0   291k      0  0:57:44  0:04:41  0:53:03  241k\n",
            "  8  987M    8 80.5M    0     0   291k      0  0:57:44  0:04:42  0:53:02  220k\n",
            "  8  987M    8 80.6M    0     0   291k      0  0:57:52  0:04:43  0:53:09  195k\n",
            "  8  987M    8 80.7M    0     0   290k      0  0:57:57  0:04:44  0:53:13  177k\n",
            "  8  987M    8 80.9M    0     0   290k      0  0:58:04  0:04:45  0:53:19  171k\n",
            "  8  987M    8 81.0M    0     0   289k      0  0:58:12  0:04:46  0:53:26  157k\n",
            "  8  987M    8 81.1M    0     0   288k      0  0:58:19  0:04:47  0:53:32  124k\n",
            "  8  987M    8 81.2M    0     0   288k      0  0:58:28  0:04:48  0:53:40  112k\n",
            "  8  987M    8 81.2M    0     0   287k      0  0:58:36  0:04:49  0:53:47  105k\n",
            "  8  987M    8 81.3M    0     0   286k      0  0:58:43  0:04:50  0:53:53   99k\n",
            "  8  987M    8 81.5M    0     0   286k      0  0:58:51  0:04:51  0:54:00  104k\n",
            "  8  987M    8 81.7M    0     0   286k      0  0:58:54  0:04:52  0:54:02  121k\n",
            "  8  987M    8 81.8M    0     0   285k      0  0:59:01  0:04:53  0:54:08  125k\n",
            "  8  987M    8 81.9M    0     0   284k      0  0:59:08  0:04:54  0:54:14  131k\n",
            "  8  987M    8 82.1M    0     0   284k      0  0:59:11  0:04:55  0:54:16  153k\n",
            "  8  987M    8 82.2M    0     0   284k      0  0:59:17  0:04:56  0:54:21  162k\n",
            "  8  987M    8 82.4M    0     0   283k      0  0:59:21  0:04:57  0:54:24  156k\n",
            "  8  987M    8 82.6M    0     0   283k      0  0:59:27  0:04:58  0:54:29  163k\n",
            "  8  987M    8 82.7M    0     0   282k      0  0:59:32  0:04:59  0:54:33  172k\n",
            "  8  987M    8 82.9M    0     0   282k      0  0:59:39  0:05:00  0:54:39  153k\n",
            "  8  987M    8 83.0M    0     0   281k      0  0:59:45  0:05:01  0:54:44  147k\n",
            "  8  987M    8 83.1M    0     0   281k      0  0:59:51  0:05:02  0:54:49  141k\n",
            "  8  987M    8 83.3M    0     0   281k      0  0:59:54  0:05:03  0:54:51  155k\n",
            "  8  987M    8 83.5M    0     0   281k      0  0:59:57  0:05:04  0:54:53  164k\n",
            "  8  987M    8 83.9M    0     0   281k      0  0:59:51  0:05:05  0:54:46  221k\n",
            "  8  987M    8 84.2M    0     0   281k      0  0:59:52  0:05:06  0:54:46  252k\n",
            "  8  987M    8 84.5M    0     0   281k      0  0:59:53  0:05:07  0:54:46  272k\n",
            "  8  987M    8 84.8M    0     0   281k      0  0:59:51  0:05:08  0:54:43  292k\n",
            "  8  987M    8 85.0M    0     0   281k      0  0:59:52  0:05:09  0:54:43  307k\n",
            "  8  987M    8 85.3M    0     0   281k      0  0:59:50  0:05:10  0:54:40  286k\n",
            "  8  987M    8 85.6M    0     0   281k      0  0:59:52  0:05:11  0:54:41  280k\n",
            "  8  987M    8 85.8M    0     0   281k      0  0:59:53  0:05:12  0:54:41  278k\n",
            "  8  987M    8 86.2M    0     0   281k      0  0:59:49  0:05:13  0:54:36  294k\n",
            "  8  987M    8 86.6M    0     0   282k      0  0:59:44  0:05:14  0:54:30  320k\n",
            "  8  987M    8 86.8M    0     0   281k      0  0:59:45  0:05:15  0:54:30  306k\n",
            "  8  987M    8 87.2M    0     0   282k      0  0:59:41  0:05:16  0:54:25  334k\n",
            "  8  987M    8 87.3M    0     0   281k      0  0:59:47  0:05:17  0:54:30  311k\n",
            "  8  987M    8 87.5M    0     0   281k      0  0:59:53  0:05:18  0:54:35  259k\n",
            "  8  987M    8 87.6M    0     0   280k      0  0:59:59  0:05:19  0:54:40  204k\n",
            "  8  987M    8 87.8M    0     0   280k      0  1:00:02  0:05:20  0:54:42  199k\n",
            "  8  987M    8 88.1M    0     0   280k      0  1:00:03  0:05:21  0:54:42  171k\n",
            "  8  987M    8 88.2M    0     0   280k      0  1:00:05  0:05:22  0:54:43  188k\n",
            "  8  987M    8 88.5M    0     0   280k      0  1:00:07  0:05:23  0:54:44  210k\n",
            "  8  987M    8 88.7M    0     0   280k      0  1:00:08  0:05:24  0:54:44  233k\n",
            "  9  987M    9 89.0M    0     0   280k      0  1:00:08  0:05:25  0:54:43  252k\n",
            "  9  987M    9 89.3M    0     0   280k      0  1:00:09  0:05:26  0:54:43  251k\n",
            "  9  987M    9 89.5M    0     0   279k      0  1:00:12  0:05:27  0:54:45  246k\n",
            "  9  987M    9 89.7M    0     0   279k      0  1:00:15  0:05:28  0:54:47  238k\n",
            "  9  987M    9 89.8M    0     0   279k      0  1:00:19  0:05:29  0:54:50  224k\n",
            "  9  987M    9 90.0M    0     0   279k      0  1:00:22  0:05:30  0:54:52  205k\n",
            "  9  987M    9 90.2M    0     0   278k      0  1:00:26  0:05:31  0:54:55  190k\n",
            "  9  987M    9 90.4M    0     0   278k      0  1:00:29  0:05:32  0:54:57  191k\n",
            "  9  987M    9 90.7M    0     0   278k      0  1:00:29  0:05:33  0:54:56  208k\n",
            "  9  987M    9 91.0M    0     0   278k      0  1:00:29  0:05:34  0:54:55  230k\n",
            "  9  987M    9 91.5M    0     0   279k      0  1:00:20  0:05:35  0:54:45  291k\n",
            "  9  987M    9 91.8M    0     0   279k      0  1:00:16  0:05:36  0:54:40  332k\n",
            "  9  987M    9 92.2M    0     0   279k      0  1:00:13  0:05:37  0:54:36  359k\n",
            "  9  987M    9 92.4M    0     0   279k      0  1:00:16  0:05:38  0:54:38  344k\n",
            "  9  987M    9 92.6M    0     0   279k      0  1:00:20  0:05:39  0:54:41  323k\n",
            "  9  987M    9 92.9M    0     0   279k      0  1:00:16  0:05:40  0:54:36  299k\n",
            "  9  987M    9 93.4M    0     0   280k      0  1:00:10  0:05:41  0:54:29  314k\n",
            "  9  987M    9 93.6M    0     0   279k      0  1:00:10  0:05:42  0:54:28  295k\n",
            "  9  987M    9 93.8M    0     0   279k      0  1:00:12  0:05:43  0:54:29  301k\n",
            "  9  987M    9 94.0M    0     0   279k      0  1:00:14  0:05:44  0:54:30  308k\n",
            "  9  987M    9 94.4M    0     0   279k      0  1:00:12  0:05:45  0:54:27  298k\n",
            "  9  987M    9 94.7M    0     0   279k      0  1:00:12  0:05:46  0:54:26  268k\n",
            "  9  987M    9 94.9M    0     0   279k      0  1:00:14  0:05:47  0:54:27  258k\n",
            "  9  987M    9 95.1M    0     0   279k      0  1:00:17  0:05:48  0:54:29  252k\n",
            "  9  987M    9 95.2M    0     0   279k      0  1:00:23  0:05:49  0:54:34  237k\n",
            "  9  987M    9 95.3M    0     0   278k      0  1:00:28  0:05:50  0:54:38  197k\n",
            "  9  987M    9 95.7M    0     0   278k      0  1:00:25  0:05:51  0:54:34  207k\n",
            "  9  987M    9 95.9M    0     0   278k      0  1:00:27  0:05:52  0:54:35  210k\n",
            "  9  987M    9 96.1M    0     0   278k      0  1:00:29  0:05:53  0:54:36  214k\n",
            "  9  987M    9 96.3M    0     0   278k      0  1:00:33  0:05:54  0:54:39  221k\n",
            "  9  987M    9 96.4M    0     0   277k      0  1:00:40  0:05:55  0:54:45  210k\n",
            "  9  987M    9 96.5M    0     0   277k      0  1:00:47  0:05:56  0:54:51  162k\n",
            "  9  987M    9 96.6M    0     0   276k      0  1:00:54  0:05:57  0:54:57  134k\n",
            "  9  987M    9 96.7M    0     0   276k      0  1:00:59  0:05:58  0:55:01  116k\n",
            "  9  987M    9 96.8M    0     0   275k      0  1:01:05  0:05:59  0:55:06  102k\n",
            "  9  987M    9 96.9M    0     0   275k      0  1:01:11  0:06:00  0:55:11  110k\n",
            "  9  987M    9 97.2M    0     0   275k      0  1:01:11  0:06:01  0:55:10  142k\n",
            "  9  987M    9 97.3M    0     0   275k      0  1:01:15  0:06:02  0:55:13  161k\n",
            "  9  987M    9 97.5M    0     0   274k      0  1:01:18  0:06:03  0:55:15  173k\n",
            "  9  987M    9 97.6M    0     0   274k      0  1:01:24  0:06:04  0:55:20  172k\n",
            "  9  987M    9 97.7M    0     0   273k      0  1:01:30  0:06:05  0:55:25  171k\n",
            "  9  987M    9 97.9M    0     0   273k      0  1:01:35  0:06:06  0:55:29  141k\n",
            "  9  987M    9 98.0M    0     0   273k      0  1:01:39  0:06:07  0:55:32  138k\n",
            "  9  987M    9 98.2M    0     0   272k      0  1:01:45  0:06:08  0:55:37  126k\n",
            "  9  987M    9 98.3M    0     0   272k      0  1:01:48  0:06:09  0:55:39  141k\n",
            "  9  987M    9 98.5M    0     0   272k      0  1:01:52  0:06:10  0:55:42  153k\n",
            " 10  987M   10 98.8M    0     0   272k      0  1:01:50  0:06:11  0:55:39  195k\n",
            " 10  987M   10 99.2M    0     0   272k      0  1:01:46  0:06:12  0:55:34  237k\n",
            " 10  987M   10 99.9M    0     0   273k      0  1:01:29  0:06:13  0:55:16  363k\n",
            " 10  987M   10  100M    0     0   275k      0  1:01:12  0:06:14  0:54:58  474k\n",
            " 10  987M   10  100M    0     0   275k      0  1:01:12  0:06:15  0:54:57  493k\n",
            " 10  987M   10  101M    0     0   275k      0  1:01:11  0:06:16  0:54:55  486k\n",
            " 10  987M   10  101M    0     0   275k      0  1:01:07  0:06:17  0:54:50  489k\n",
            " 10  987M   10  101M    0     0   275k      0  1:01:06  0:06:18  0:54:48  406k\n",
            " 10  987M   10  102M    0     0   275k      0  1:01:03  0:06:19  0:54:44  323k\n",
            " 10  987M   10  102M    0     0   276k      0  1:00:55  0:06:20  0:54:35  373k\n",
            " 10  987M   10  103M    0     0   276k      0  1:00:53  0:06:21  0:54:32  378k\n",
            " 10  987M   10  103M    0     0   277k      0  1:00:47  0:06:22  0:54:25  390k\n",
            " 10  987M   10  103M    0     0   277k      0  1:00:45  0:06:23  0:54:22  393k\n",
            " 10  987M   10  104M    0     0   277k      0  1:00:42  0:06:24  0:54:18  396k\n",
            " 10  987M   10  104M    0     0   277k      0  1:00:48  0:06:25  0:54:23  317k\n",
            " 10  987M   10  104M    0     0   276k      0  1:00:51  0:06:26  0:54:25  290k\n",
            " 10  987M   10  104M    0     0   276k      0  1:00:54  0:06:27  0:54:27  235k\n",
            " 10  987M   10  104M    0     0   276k      0  1:00:58  0:06:28  0:54:30  202k\n",
            " 10  987M   10  104M    0     0   275k      0  1:01:03  0:06:29  0:54:34  159k\n",
            " 10  987M   10  105M    0     0   275k      0  1:01:04  0:06:30  0:54:34  184k\n",
            " 10  987M   10  105M    0     0   275k      0  1:01:04  0:06:31  0:54:33  201k\n",
            " 10  987M   10  105M    0     0   275k      0  1:01:07  0:06:32  0:54:35  205k\n",
            " 10  987M   10  105M    0     0   275k      0  1:01:08  0:06:33  0:54:35  220k\n",
            " 10  987M   10  106M    0     0   276k      0  1:00:59  0:06:34  0:54:25  294k\n",
            " 10  987M   10  106M    0     0   276k      0  1:00:54  0:06:35  0:54:19  331k\n",
            " 10  987M   10  107M    0     0   277k      0  1:00:47  0:06:36  0:54:11  375k\n",
            " 10  987M   10  107M    0     0   277k      0  1:00:43  0:06:37  0:54:06  415k\n",
            " 10  987M   10  108M    0     0   277k      0  1:00:37  0:06:38  0:53:59  461k\n",
            " 11  987M   11  108M    0     0   278k      0  1:00:27  0:06:39  0:53:48  475k\n",
            " 11  987M   11  109M    0     0   280k      0  1:00:07  0:06:40  0:53:27  567k\n",
            " 11  987M   11  110M    0     0   281k      0  0:59:56  0:06:41  0:53:15  592k\n",
            " 11  987M   11  110M    0     0   281k      0  0:59:48  0:06:42  0:53:06  619k\n",
            " 11  987M   11  111M    0     0   282k      0  0:59:43  0:06:43  0:53:00  620k\n",
            " 11  987M   11  111M    0     0   282k      0  0:59:32  0:06:44  0:52:48  623k\n",
            " 11  987M   11  112M    0     0   283k      0  0:59:19  0:06:45  0:52:34  580k\n",
            " 11  987M   11  112M    0     0   284k      0  0:59:19  0:06:46  0:52:33  521k\n",
            " 11  987M   11  113M    0     0   284k      0  0:59:19  0:06:47  0:52:32  477k\n",
            " 11  987M   11  113M    0     0   283k      0  0:59:20  0:06:48  0:52:32  432k\n",
            " 11  987M   11  113M    0     0   284k      0  0:59:17  0:06:49  0:52:28  383k\n",
            " 11  987M   11  113M    0     0   283k      0  0:59:21  0:06:50  0:52:31  271k\n",
            " 11  987M   11  113M    0     0   283k      0  0:59:24  0:06:51  0:52:33  247k\n",
            " 11  987M   11  114M    0     0   283k      0  0:59:29  0:06:52  0:52:37  214k\n",
            " 11  987M   11  114M    0     0   282k      0  0:59:33  0:06:53  0:52:40  197k\n",
            " 11  987M   11  114M    0     0   282k      0  0:59:36  0:06:54  0:52:42  157k\n",
            " 11  987M   11  114M    0     0   282k      0  0:59:37  0:06:55  0:52:42  182k\n",
            " 11  987M   11  114M    0     0   282k      0  0:59:39  0:06:56  0:52:43  188k\n",
            " 11  987M   11  115M    0     0   282k      0  0:59:42  0:06:57  0:52:45  205k\n",
            " 11  987M   11  115M    0     0   282k      0  0:59:44  0:06:58  0:52:46  214k\n",
            " 11  987M   11  115M    0     0   281k      0  0:59:46  0:06:59  0:52:47  219k\n",
            " 11  987M   11  115M    0     0   281k      0  0:59:46  0:07:00  0:52:46  221k\n",
            " 11  987M   11  116M    0     0   281k      0  0:59:45  0:07:01  0:52:44  242k\n",
            " 11  987M   11  116M    0     0   282k      0  0:59:39  0:07:02  0:52:37  296k\n",
            " 11  987M   11  116M    0     0   282k      0  0:59:39  0:07:03  0:52:36  309k\n",
            " 11  987M   11  116M    0     0   282k      0  0:59:42  0:07:04  0:52:38  305k\n",
            " 11  987M   11  117M    0     0   282k      0  0:59:39  0:07:05  0:52:34  332k\n",
            " 11  987M   11  117M    0     0   282k      0  0:59:36  0:07:06  0:52:30  338k\n",
            " 11  987M   11  118M    0     0   282k      0  0:59:33  0:07:07  0:52:26  324k\n",
            " 11  987M   11  118M    0     0   282k      0  0:59:33  0:07:08  0:52:25  329k\n",
            " 12  987M   12  118M    0     0   282k      0  0:59:34  0:07:09  0:52:25  336k\n",
            " 12  987M   12  118M    0     0   282k      0  0:59:39  0:07:10  0:52:29  280k\n",
            " 12  987M   12  119M    0     0   282k      0  0:59:40  0:07:11  0:52:29  257k\n",
            " 12  987M   12  119M    0     0   282k      0  0:59:41  0:07:12  0:52:29  231k\n",
            " 12  987M   12  119M    0     0   282k      0  0:59:41  0:07:13  0:52:28  225k\n",
            " 12  987M   12  119M    0     0   282k      0  0:59:43  0:07:14  0:52:29  224k\n",
            " 12  987M   12  120M    0     0   282k      0  0:59:43  0:07:15  0:52:28  257k\n",
            " 12  987M   12  120M    0     0   282k      0  0:59:40  0:07:16  0:52:24  284k\n",
            " 12  987M   12  120M    0     0   282k      0  0:59:35  0:07:17  0:52:18  320k\n",
            " 12  987M   12  121M    0     0   282k      0  0:59:36  0:07:18  0:52:18  317k\n",
            " 12  987M   12  121M    0     0   282k      0  0:59:38  0:07:19  0:52:19  313k\n",
            " 12  987M   12  121M    0     0   282k      0  0:59:42  0:07:20  0:52:22  289k\n",
            " 12  987M   12  121M    0     0   281k      0  0:59:46  0:07:21  0:52:25  243k\n",
            " 12  987M   12  121M    0     0   281k      0  0:59:49  0:07:22  0:52:27  189k\n",
            " 12  987M   12  121M    0     0   281k      0  0:59:50  0:07:23  0:52:27  182k\n",
            " 12  987M   12  122M    0     0   281k      0  0:59:52  0:07:24  0:52:28  185k\n",
            " 12  987M   12  122M    0     0   281k      0  0:59:55  0:07:25  0:52:30  187k\n",
            " 12  987M   12  122M    0     0   280k      0  1:00:00  0:07:26  0:52:34  181k\n",
            " 12  987M   12  122M    0     0   280k      0  1:00:02  0:07:27  0:52:35  184k\n",
            " 12  987M   12  122M    0     0   280k      0  1:00:07  0:07:28  0:52:39  165k\n",
            " 12  987M   12  122M    0     0   279k      0  1:00:12  0:07:29  0:52:43  147k\n",
            " 12  987M   12  122M    0     0   279k      0  1:00:16  0:07:30  0:52:46  131k\n",
            " 12  987M   12  123M    0     0   279k      0  1:00:20  0:07:31  0:52:49  140k\n",
            " 12  987M   12  123M    0     0   279k      0  1:00:22  0:07:32  0:52:50  146k\n",
            " 12  987M   12  123M    0     0   278k      0  1:00:23  0:07:33  0:52:50  166k\n",
            " 12  987M   12  123M    0     0   278k      0  1:00:24  0:07:34  0:52:50  196k\n",
            " 12  987M   12  124M    0     0   278k      0  1:00:25  0:07:35  0:52:50  218k\n",
            " 12  987M   12  124M    0     0   279k      0  1:00:22  0:07:36  0:52:46  261k\n",
            " 12  987M   12  124M    0     0   278k      0  1:00:24  0:07:37  0:52:47  263k\n",
            " 12  987M   12  125M    0     0   279k      0  1:00:19  0:07:38  0:52:41  310k\n",
            " 12  987M   12  125M    0     0   279k      0  1:00:11  0:07:39  0:52:32  368k\n",
            " 12  987M   12  126M    0     0   280k      0  1:00:07  0:07:40  0:52:27  404k\n",
            " 12  987M   12  126M    0     0   280k      0  1:00:05  0:07:41  0:52:24  403k\n",
            " 12  987M   12  126M    0     0   280k      0  1:00:06  0:07:42  0:52:24  411k\n",
            " 12  987M   12  126M    0     0   280k      0  1:00:04  0:07:43  0:52:21  382k\n",
            " 12  987M   12  127M    0     0   280k      0  1:00:04  0:07:44  0:52:20  333k\n",
            " 12  987M   12  127M    0     0   280k      0  1:00:01  0:07:45  0:52:16  323k\n",
            " 12  987M   12  128M    0     0   281k      0  0:59:55  0:07:46  0:52:09  353k\n",
            " 13  987M   13  128M    0     0   281k      0  0:59:53  0:07:47  0:52:06  369k\n",
            " 13  987M   13  128M    0     0   281k      0  0:59:53  0:07:48  0:52:05  358k\n",
            " 13  987M   13  129M    0     0   281k      0  0:59:52  0:07:49  0:52:03  371k\n",
            " 13  987M   13  129M    0     0   281k      0  0:59:51  0:07:50  0:52:01  355k\n",
            " 13  987M   13  129M    0     0   281k      0  0:59:52  0:07:51  0:52:01  299k\n",
            " 13  987M   13  129M    0     0   281k      0  0:59:56  0:07:52  0:52:04  264k\n",
            " 13  987M   13  130M    0     0   281k      0  0:59:55  0:07:53  0:52:02  267k\n",
            " 13  987M   13  130M    0     0   281k      0  0:59:56  0:07:54  0:52:02  247k\n",
            " 13  987M   13  130M    0     0   280k      0  0:59:59  0:07:55  0:52:04  222k\n",
            " 13  987M   13  130M    0     0   280k      0  1:00:01  0:07:56  0:52:05  217k\n",
            " 13  987M   13  130M    0     0   280k      0  1:00:03  0:07:57  0:52:06  227k\n",
            " 13  987M   13  131M    0     0   280k      0  1:00:05  0:07:58  0:52:07  204k\n",
            " 13  987M   13  131M    0     0   280k      0  1:00:04  0:07:59  0:52:05  226k\n",
            " 13  987M   13  131M    0     0   280k      0  1:00:02  0:08:00  0:52:02  259k\n",
            " 13  987M   13  131M    0     0   280k      0  1:00:05  0:08:01  0:52:04  255k\n",
            " 13  987M   13  132M    0     0   280k      0  1:00:08  0:08:02  0:52:06  244k\n",
            " 13  987M   13  132M    0     0   280k      0  1:00:08  0:08:03  0:52:05  262k\n",
            " 13  987M   13  132M    0     0   280k      0  1:00:10  0:08:04  0:52:06  236k\n",
            " 13  987M   13  132M    0     0   279k      0  1:00:13  0:08:05  0:52:08  200k\n",
            " 13  987M   13  132M    0     0   279k      0  1:00:15  0:08:06  0:52:09  203k\n",
            " 13  987M   13  133M    0     0   279k      0  1:00:16  0:08:07  0:52:09  220k\n",
            " 13  987M   13  133M    0     0   279k      0  1:00:13  0:08:08  0:52:05  235k\n",
            " 13  987M   13  133M    0     0   279k      0  1:00:13  0:08:09  0:52:04  251k\n",
            " 13  987M   13  134M    0     0   279k      0  1:00:13  0:08:10  0:52:03  281k\n",
            " 13  987M   13  134M    0     0   279k      0  1:00:11  0:08:11  0:52:00  308k\n",
            " 13  987M   13  134M    0     0   279k      0  1:00:11  0:08:12  0:51:59  319k\n",
            " 13  987M   13  135M    0     0   280k      0  1:00:02  0:08:13  0:51:49  372k\n",
            " 13  987M   13  135M    0     0   280k      0  1:00:01  0:08:14  0:51:47  376k\n",
            " 13  987M   13  135M    0     0   280k      0  1:00:01  0:08:15  0:51:46  366k\n",
            " 13  987M   13  136M    0     0   280k      0  1:00:03  0:08:16  0:51:47  342k\n",
            " 13  987M   13  136M    0     0   280k      0  1:00:03  0:08:17  0:51:46  339k\n",
            " 13  987M   13  136M    0     0   280k      0  1:00:02  0:08:18  0:51:44  278k\n",
            " 13  987M   13  136M    0     0   280k      0  1:00:02  0:08:19  0:51:43  271k\n",
            " 13  987M   13  137M    0     0   280k      0  1:00:01  0:08:20  0:51:41  279k\n",
            " 13  987M   13  137M    0     0   280k      0  1:00:01  0:08:21  0:51:40  290k\n",
            " 13  987M   13  137M    0     0   280k      0  1:00:01  0:08:22  0:51:39  294k\n",
            " 13  987M   13  137M    0     0   280k      0  1:00:03  0:08:23  0:51:40  275k\n",
            " 14  987M   14  138M    0     0   280k      0  1:00:00  0:08:24  0:51:36  291k\n",
            " 14  987M   14  138M    0     0   280k      0  1:00:02  0:08:25  0:51:37  272k\n",
            " 14  987M   14  138M    0     0   280k      0  1:00:05  0:08:26  0:51:39  250k\n",
            " 14  987M   14  138M    0     0   280k      0  1:00:07  0:08:27  0:51:40  232k\n",
            " 14  987M   14  139M    0     0   280k      0  1:00:04  0:08:28  0:51:36  266k\n",
            " 14  987M   14  139M    0     0   280k      0  1:00:06  0:08:29  0:51:37  235k\n",
            " 14  987M   14  139M    0     0   280k      0  1:00:08  0:08:30  0:51:38  236k\n",
            " 14  987M   14  139M    0     0   279k      0  1:00:12  0:08:31  0:51:41  229k\n",
            " 14  987M   14  139M    0     0   279k      0  1:00:15  0:08:32  0:51:43  216k\n",
            " 14  987M   14  140M    0     0   279k      0  1:00:18  0:08:33  0:51:45  175k\n",
            " 14  987M   14  140M    0     0   279k      0  1:00:17  0:08:34  0:51:43  192k\n",
            " 14  987M   14  140M    0     0   279k      0  1:00:19  0:08:35  0:51:44  192k\n",
            " 14  987M   14  140M    0     0   279k      0  1:00:20  0:08:36  0:51:44  212k\n",
            " 14  987M   14  141M    0     0   279k      0  1:00:20  0:08:37  0:51:43  241k\n",
            " 14  987M   14  141M    0     0   279k      0  1:00:22  0:08:38  0:51:44  245k\n",
            " 14  987M   14  141M    0     0   278k      0  1:00:23  0:08:39  0:51:44  227k\n",
            " 14  987M   14  141M    0     0   278k      0  1:00:25  0:08:40  0:51:45  229k\n",
            " 14  987M   14  142M    0     0   279k      0  1:00:22  0:08:41  0:51:41  262k\n",
            " 14  987M   14  142M    0     0   279k      0  1:00:22  0:08:42  0:51:40  263k\n",
            " 14  987M   14  142M    0     0   278k      0  1:00:25  0:08:43  0:51:42  258k\n",
            " 14  987M   14  142M    0     0   278k      0  1:00:28  0:08:44  0:51:44  243k\n",
            " 14  987M   14  142M    0     0   278k      0  1:00:33  0:08:45  0:51:48  215k\n",
            " 14  987M   14  142M    0     0   277k      0  1:00:38  0:08:46  0:51:52  151k\n",
            " 14  987M   14  142M    0     0   277k      0  1:00:43  0:08:47  0:51:56  111k\n",
            " 14  987M   14  143M    0     0   277k      0  1:00:48  0:08:48  0:52:00 93937\n",
            " 14  987M   14  143M    0     0   276k      0  1:00:52  0:08:49  0:52:03 86670\n",
            " 14  987M   14  143M    0     0   276k      0  1:00:54  0:08:50  0:52:04  106k\n",
            " 14  987M   14  143M    0     0   276k      0  1:00:56  0:08:51  0:52:05  132k\n",
            " 14  987M   14  143M    0     0   276k      0  1:00:58  0:08:52  0:52:06  157k\n",
            " 14  987M   14  143M    0     0   276k      0  1:00:58  0:08:53  0:52:05  192k\n",
            " 14  987M   14  144M    0     0   276k      0  1:00:54  0:08:54  0:52:00  262k\n",
            " 14  987M   14  145M    0     0   277k      0  1:00:38  0:08:55  0:51:43  402k\n",
            " 14  987M   14  146M    0     0   280k      0  1:00:08  0:08:56  0:51:12  680k\n",
            " 15  987M   15  149M    0     0   283k      0  0:59:21  0:08:57  0:50:24 1084k\n",
            " 15  987M   15  150M    0     0   285k      0  0:58:55  0:08:58  0:49:57 1315k\n",
            " 15  987M   15  152M    0     0   289k      0  0:58:09  0:08:59  0:49:10 1687k\n",
            " 15  987M   15  155M    0     0   294k      0  0:57:18  0:09:00  0:48:18 2027k\n",
            " 16  987M   16  158M    0     0   299k      0  0:56:12  0:09:01  0:47:11 2396k\n",
            " 16  987M   16  161M    0     0   305k      0  0:55:10  0:09:02  0:46:08 2622k\n",
            " 16  987M   16  165M    0     0   311k      0  0:54:08  0:09:03  0:45:05 3033k\n",
            " 17  987M   17  168M    0     0   317k      0  0:53:04  0:09:04  0:44:00 3306k\n",
            " 17  987M   17  172M    0     0   323k      0  0:52:03  0:09:05  0:42:58 3531k\n",
            " 17  987M   17  175M    0     0   328k      0  0:51:17  0:09:06  0:42:11 3444k\n",
            " 18  987M   18  178M    0     0   333k      0  0:50:27  0:09:07  0:41:20 3434k\n",
            " 18  987M   18  180M    0     0   337k      0  0:49:53  0:09:08  0:40:45 3217k\n",
            " 18  987M   18  185M    0     0   345k      0  0:48:45  0:09:09  0:39:36 3413k\n",
            " 19  987M   19  192M    0     0   358k      0  0:47:03  0:09:10  0:37:53 4110k\n",
            " 20  987M   20  201M    0     0   373k      0  0:45:03  0:09:11  0:35:52 5361k\n",
            " 21  987M   21  210M    0     0   390k      0  0:43:10  0:09:12  0:33:58 6565k\n",
            " 22  987M   22  218M    0     0   404k      0  0:41:39  0:09:13  0:32:26 7727k\n",
            " 23  987M   23  227M    0     0   420k      0  0:40:02  0:09:14  0:30:48 8682k\n",
            " 23  987M   23  236M    0     0   436k      0  0:38:38  0:09:15  0:29:23 9024k\n",
            " 24  987M   24  244M    0     0   449k      0  0:37:27  0:09:16  0:28:11 8802k\n",
            " 25  987M   25  254M    0     0   466k      0  0:36:06  0:09:17  0:26:49 8889k\n",
            " 26  987M   26  261M    0     0   478k      0  0:35:10  0:09:18  0:25:52 8732k\n",
            " 27  987M   27  269M    0     0   493k      0  0:34:09  0:09:19  0:24:50 8541k\n",
            " 28  987M   28  276M    0     0   505k      0  0:33:19  0:09:20  0:23:59 8220k\n",
            " 28  987M   28  284M    0     0   519k      0  0:32:25  0:09:21  0:23:04 8279k\n",
            " 29  987M   29  292M    0     0   533k      0  0:31:36  0:09:22  0:22:14 7947k\n",
            " 30  987M   30  301M    0     0   547k      0  0:30:45  0:09:23  0:21:22 8234k\n",
            " 31  987M   31  310M    0     0   562k      0  0:29:57  0:09:24  0:20:33 8296k\n",
            " 32  987M   32  318M    0     0   576k      0  0:29:12  0:09:25  0:19:47 8519k\n",
            " 32  987M   32  325M    0     0   588k      0  0:28:36  0:09:26  0:19:10 8382k\n",
            " 33  987M   33  333M    0     0   601k      0  0:28:00  0:09:27  0:18:33 8297k\n",
            " 34  987M   34  340M    0     0   613k      0  0:27:26  0:09:28  0:17:58 8062k\n",
            " 35  987M   35  348M    0     0   626k      0  0:26:54  0:09:29  0:17:25 7801k\n",
            " 36  987M   36  356M    0     0   640k      0  0:26:18  0:09:30  0:16:48 7885k\n",
            " 36  987M   36  363M    0     0   650k      0  0:25:53  0:09:31  0:16:22 7657k\n",
            " 37  987M   37  369M    0     0   661k      0  0:25:28  0:09:32  0:15:56 7442k\n",
            " 38  987M   38  377M    0     0   673k      0  0:25:00  0:09:33  0:15:27 7479k\n",
            " 38  987M   38  384M    0     0   685k      0  0:24:34  0:09:34  0:15:00 7494k\n",
            " 39  987M   39  392M    0     0   698k      0  0:24:07  0:09:35  0:14:32 7306k\n",
            " 40  987M   40  398M    0     0   707k      0  0:23:48  0:09:36  0:14:12 7200k\n",
            " 40  987M   40  404M    0     0   717k      0  0:23:28  0:09:37  0:13:51 7176k\n",
            " 41  987M   41  411M    0     0   728k      0  0:23:07  0:09:38  0:13:29 7011k\n",
            " 42  987M   42  419M    0     0   741k      0  0:22:43  0:09:39  0:13:04 7090k\n",
            " 43  987M   43  427M    0     0   754k      0  0:22:20  0:09:40  0:12:40 7163k\n",
            " 43  987M   43  434M    0     0   764k      0  0:22:01  0:09:41  0:12:20 7372k\n",
            " 44  987M   44  438M    0     0   771k      0  0:21:50  0:09:42  0:12:08 6959k\n",
            " 45  987M   45  449M    0     0   788k      0  0:21:22  0:09:43  0:11:39 7695k\n",
            " 46  987M   46  463M    0     0   811k      0  0:20:46  0:09:44  0:11:02 8964k\n",
            " 48  987M   48  474M    0     0   829k      0  0:20:18  0:09:45  0:10:33 9580k\n",
            " 49  987M   49  489M    0     0   855k      0  0:19:42  0:09:46  0:09:56 11.0M\n",
            " 51  987M   51  506M    0     0   882k      0  0:19:05  0:09:47  0:09:18 13.5M\n",
            " 52  987M   52  521M    0     0   907k      0  0:18:34  0:09:48  0:08:46 14.3M\n",
            " 54  987M   54  534M    0     0   928k      0  0:18:09  0:09:49  0:08:20 14.2M\n",
            " 55  987M   55  547M    0     0   949k      0  0:17:44  0:09:50  0:07:54 14.6M\n",
            " 56  987M   56  560M    0     0   970k      0  0:17:21  0:09:51  0:07:30 14.1M\n",
            " 57  987M   57  572M    0     0   989k      0  0:17:02  0:09:52  0:07:10 13.2M\n",
            " 58  987M   58  582M    0     0  1004k      0  0:16:46  0:09:53  0:06:53 12.2M\n",
            " 59  987M   59  591M    0     0  1018k      0  0:16:33  0:09:54  0:06:39 11.3M\n",
            " 61  987M   61  602M    0     0  1036k      0  0:16:15  0:09:55  0:06:20 10.9M\n",
            " 61  987M   61  610M    0     0  1047k      0  0:16:05  0:09:56  0:06:09  9.8M\n",
            " 62  987M   62  618M    0     0  1059k      0  0:15:53  0:09:57  0:05:56 9384k\n",
            " 63  987M   63  624M    0     0  1068k      0  0:15:46  0:09:58  0:05:48 8671k\n",
            " 63  987M   63  627M    0     0  1071k      0  0:15:43  0:09:59  0:05:44 7366k\n",
            " 64  987M   64  639M    0     0  1090k      0  0:15:27  0:10:00  0:05:27 7525k\n",
            " 66  987M   66  652M    0     0  1109k      0  0:15:10  0:10:01  0:05:09 8591k\n",
            " 67  987M   67  666M    0     0  1132k      0  0:14:52  0:10:02  0:04:50 9848k\n",
            " 69  987M   69  682M    0     0  1158k      0  0:14:32  0:10:03  0:04:29 11.6M\n",
            " 70  987M   70  698M    0     0  1183k      0  0:14:14  0:10:04  0:04:10 14.3M\n",
            " 72  987M   72  711M    0     0  1203k      0  0:13:59  0:10:05  0:03:54 14.4M\n",
            " 73  987M   73  726M    0     0  1227k      0  0:13:43  0:10:06  0:03:37 15.0M\n",
            " 75  987M   75  743M    0     0  1252k      0  0:13:27  0:10:07  0:03:20 15.2M\n",
            " 76  987M   76  753M    0     0  1267k      0  0:13:17  0:10:08  0:03:09 14.0M\n",
            " 77  987M   77  763M    0     0  1282k      0  0:13:08  0:10:09  0:02:59 12.8M\n",
            " 78  987M   78  773M    0     0  1298k      0  0:12:58  0:10:10  0:02:48 12.5M\n",
            " 79  987M   79  784M    0     0  1313k      0  0:12:49  0:10:11  0:02:38 11.4M\n",
            " 80  987M   80  794M    0     0  1328k      0  0:12:41  0:10:12  0:02:29 10.3M\n",
            " 81  987M   81  804M    0     0  1342k      0  0:12:32  0:10:13  0:02:19 10.1M\n",
            " 82  987M   82  813M    0     0  1354k      0  0:12:26  0:10:14  0:02:12  9.9M\n",
            " 83  987M   83  822M    0     0  1368k      0  0:12:18  0:10:15  0:02:03 9968k\n",
            " 84  987M   84  833M    0     0  1385k      0  0:12:09  0:10:16  0:01:53  9.8M\n",
            " 85  987M   85  844M    0     0  1400k      0  0:12:01  0:10:17  0:01:44  9.9M\n",
            " 86  987M   86  852M    0     0  1410k      0  0:11:56  0:10:18  0:01:38 9864k\n",
            " 87  987M   87  860M    0     0  1422k      0  0:11:50  0:10:19  0:01:31 9728k\n",
            " 88  987M   88  872M    0     0  1439k      0  0:11:42  0:10:20  0:01:22  9.8M\n",
            " 89  987M   89  885M    0     0  1458k      0  0:11:33  0:10:21  0:01:12 10.3M\n",
            " 91  987M   91  902M    0     0  1485k      0  0:11:20  0:10:22  0:00:58 11.7M\n",
            " 92  987M   92  913M    0     0  1500k      0  0:11:13  0:10:23  0:00:50 12.2M\n",
            " 93  987M   93  926M    0     0  1518k      0  0:11:05  0:10:24  0:00:41 13.1M\n",
            " 95  987M   95  938M    0     0  1536k      0  0:10:58  0:10:25  0:00:33 13.2M\n",
            " 96  987M   96  950M    0     0  1554k      0  0:10:50  0:10:26  0:00:24 13.0M\n",
            " 98  987M   98  969M    0     0  1581k      0  0:10:39  0:10:27  0:00:12 12.9M\n",
            " 99  987M   99  980M    0     0  1598k      0  0:10:32  0:10:28  0:00:04 13.5M\n",
            "100  987M  100  987M    0     0  1607k      0  0:10:28  0:10:28 --:--:-- 13.6M\n"
          ]
        }
      ],
      "source": [
        "print(\"Getting dataset: START\")\n",
        "%cd $WORK_DIR/xcs231n/datasets\n",
        "!bash get_datasets.sh\n",
        "%cd $WORK_DIR\n",
        "print(\"Getting dataset: DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d76b05d",
      "metadata": {},
      "source": [
        "# Package Manager Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eefd455",
      "metadata": {},
      "source": [
        "Below you have the option to run through the installation of `uv` package manager. Optionally, you can still work with `conda`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef618f0e",
      "metadata": {},
      "source": [
        "## `uv` (preferred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774a8631",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not IN_COLAB:\n",
        "  %cd $WORK_DIR/../..\n",
        "  !source install.sh -r\n",
        "  !source .venv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc823161",
      "metadata": {},
      "source": [
        "## `conda` (legacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161ae8ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment to use conda\n",
        "# if IN_COLAB:\n",
        "#     !pip install -q condacolab\n",
        "#     import condacolab\n",
        "#     condacolab.install()\n",
        "#     %cd $WORK_DIR/src\n",
        "#     !conda env create -f environment.yml\n",
        "#     !exec bash\n",
        "# else:\n",
        "#     %cd $WORK_DIR/src\n",
        "#     !conda env create -f environment.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_divider",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assignment_content_header",
      "metadata": {},
      "source": [
        "<a id=\"assignment-content\"></a>\n",
        "\n",
        "# Assignment Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvYckCI7qyX",
        "tags": [
          "pdf-title"
        ]
      },
      "source": [
        "# Image Captioning with Transformers\n",
        "You have now implemented a vanilla RNN and for the task of image captioning. In this notebook you will implement key pieces of a transformer decoder to accomplish the same task.\n",
        "\n",
        "**NOTE:** This notebook will be primarily written in PyTorch rather than NumPy, unlike the RNN notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXCykho37qya",
        "outputId": "2bc1a2b7-dfa3-4c53-d04b-f336af97ab0b",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "# Setup cell.\n",
        "import time, os, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from xcs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from xcs231n.transformer_layers import *\n",
        "from xcs231n.captioning_solver_transformer import CaptioningSolverTransformer\n",
        "from xcs231n.classifiers.transformer import CaptioningTransformer\n",
        "from xcs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
        "from xcs231n.image_utils import image_from_url\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fczxReWM7qyd"
      },
      "source": [
        "# COCO Dataset\n",
        "As in the previous notebooks, we will use the COCO dataset for captioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCKchFVJ7qye",
        "outputId": "363576b7-cc41-4130-d472-a08d602489a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base dir  D:\\course\\XCS231N-Course\\XCS231N-A3\\src\\submission\\xcs231n\\datasets/coco_captioning\n",
            "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
            "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
            "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
            "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
            "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
            "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
            "idx_to_word <class 'list'> 1004\n",
            "word_to_idx <class 'dict'> 1004\n",
            "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
            "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
          ]
        }
      ],
      "source": [
        "# Load COCO data from disk into a dictionary.\n",
        "data = load_coco_data(pca_features=True)\n",
        "\n",
        "# Print out all the keys and values from the data dictionary.\n",
        "for k, v in data.items():\n",
        "    if type(v) == np.ndarray:\n",
        "        print(k, type(v), v.shape, v.dtype)\n",
        "    else:\n",
        "        print(k, type(v), len(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go1XDgIn7qyg"
      },
      "source": [
        "# Transformer\n",
        "As you have seen, RNNs are incredibly powerful but often slow to train. Further, RNNs struggle to encode long-range dependencies (though LSTMs are one way of mitigating the issue). In 2017, Vaswani et al introduced the Transformer in their paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) to a) introduce parallelism and b) allow models to learn long-range dependencies. The paper not only led to famous models like BERT and GPT in the natural language processing community, but also an explosion of interest across fields, including vision. While here we introduce the model in the context of image captioning, the idea of attention itself is much more general.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPMDm4F9m0v"
      },
      "source": [
        "# Transformer: Multi-Headed Attention\n",
        "\n",
        "### Dot-Product Attention\n",
        "\n",
        "Recall that attention can be viewed as an operation on a query $q\\in\\mathbb{R}^d$, a set of value vectors $\\{v_1,\\dots,v_n\\}, v_i\\in\\mathbb{R}^d$, and a set of key vectors $\\{k_1,\\dots,k_n\\}, k_i \\in \\mathbb{R}^d$, specified as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91g6XLTr7qyi"
      },
      "source": [
        "\\begin{align}\n",
        "\\alpha_i &= \\frac{\\exp(k_i^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)} \\\\\n",
        "c &= \\sum_{i=1}^{n} v_i \\alpha_i \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2wcdeS7qyj"
      },
      "source": [
        "where $\\alpha_i$ are frequently called the \"attention weights\", and the output $c\\in\\mathbb{R}^d$ is a correspondingly weighted average over the value vectors.\n",
        "\n",
        "### Self-Attention\n",
        "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V,K,Q \\in \\mathbb{R}^{d\\times d}$ to map our input $X$ as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2JDsjFU7qyl"
      },
      "source": [
        "\\begin{align}\n",
        "v_i = Vx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
        "k_i = Kx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
        "q_i = Qx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5GNjXoW7qyn"
      },
      "source": [
        "### Multi-Headed Scaled Dot-Product Attention\n",
        "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $h$ be number of heads, and $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRyHp98f7qyo"
      },
      "source": [
        "\\begin{equation}\n",
        "Y_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)(XV_i)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzKHMc3g7qyo"
      },
      "source": [
        "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n",
        "\n",
        "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJFFO4uu7qyp"
      },
      "source": [
        "\\begin{equation}\n",
        "Y_i = \\text{dropout}\\bigg(\\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\\bigg)(XV_i)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlyhrBW7qyp"
      },
      "source": [
        "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFR0osPf7qyq"
      },
      "source": [
        "\\begin{equation}\n",
        "Y = [Y_1;\\dots;Y_h]A\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4G9fKfW7qyq"
      },
      "source": [
        "were $A \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$.\n",
        "\n",
        "Implement multi-headed scaled dot-product attention in the `MultiHeadAttention` class in the file `xcs231n/transformer_layers.py`. The code below will check your implementation. The relative error should be less than `e-3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeixCEKF7qyr",
        "outputId": "3025b07e-a8b0-46a3-cfc0-ae789e496fad",
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "\n",
        "# Choose dimensions such that they are all unique for easier debugging:\n",
        "# Specifically, the following values correspond to N=1, H=2, T=3, E//H=4, and E=8.\n",
        "batch_size = 1\n",
        "sequence_length = 3\n",
        "embed_dim = 8\n",
        "attn = MultiHeadAttention(embed_dim, num_heads=2)\n",
        "\n",
        "# Self-attention.\n",
        "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "self_attn_output = attn(query=data, key=data, value=data)\n",
        "\n",
        "# Masked self-attention.\n",
        "mask = torch.randn(sequence_length, sequence_length) < 0.5\n",
        "masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)\n",
        "\n",
        "# Attention using two inputs.\n",
        "other_data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "attn_output = attn(query=data, key=other_data, value=other_data)\n",
        "\n",
        "expected_self_attn_output = np.asarray([[[-0.1148,  0.2249,  0.7990, -0.4306, -0.3181,  0.2581, -0.2803,\n",
        "          -0.3003],\n",
        "         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,\n",
        "          -0.2476],\n",
        "         [-0.1645,  0.1524,  0.6942, -0.2624, -0.3221,  0.1781, -0.3361,\n",
        "          -0.1662]]])\n",
        "\n",
        "expected_masked_self_attn_output = np.asarray([[[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,\n",
        "          -0.3019],\n",
        "         [-0.5323, -0.0602,  0.6767, -0.3014,  0.2449,  0.4311,  0.0079,\n",
        "          -0.1761],\n",
        "         [-0.1183,  0.2395,  0.7118, -0.2996, -0.5005,  0.1880, -0.3792,\n",
        "          -0.2138]]])\n",
        "    \n",
        "\n",
        "expected_attn_output = np.asarray(\n",
        "  [[[ 0.1660,  0.3670,  0.2343, -0.4003, -0.0822,  0.1100, -0.4037,\n",
        "          -0.1459],\n",
        "         [ 0.2094,  0.3884,  0.4454, -0.4709, -0.1731,  0.0913, -0.4181,\n",
        "          -0.2077],\n",
        "         [ 0.1936,  0.2913,  0.4173, -0.4385, -0.1120,  0.0736, -0.4017,\n",
        "          -0.1803]]])\n",
        "\n",
        "\n",
        "print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))\n",
        "print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))\n",
        "print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDBRnqL9m0w"
      },
      "source": [
        "# Positional Encoding\n",
        "\n",
        "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
        "\n",
        "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zawv8vzV7qyt"
      },
      "source": [
        "$$\n",
        "\\begin{cases}\n",
        "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
        "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0fhlupT7qyt"
      },
      "source": [
        "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$.\n",
        "\n",
        "Implement this layer in `PositionalEncoding` in `xcs231n/transformer_layers.py`. Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `e-3` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi7px0XK7qyu",
        "outputId": "0ea7dff0-fab5-4487-e225-991aa1389cf3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = 2\n",
        "embed_dim = 6\n",
        "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "\n",
        "pos_encoder = PositionalEncoding(embed_dim)\n",
        "output = pos_encoder(data)\n",
        "\n",
        "expected_pe_output = np.asarray([[[-1.234,  1.113,  1.698, -0.086, -0.774,  1.273],\n",
        "         [ 0.903, -0.478,  0.553,  0.813,  1.264,  1.703]]])\n",
        "\n",
        "print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDoSUJ7y7qyv",
        "tags": [
          "pdf-inline"
        ]
      },
      "source": [
        "# Inline Question 1\n",
        "\n",
        "Several key design decisions were made in designing the scaled dot product attention we introduced above. Explain why the following choices were beneficial:\n",
        "1. Using multiple attention heads as opposed to one.\n",
        "2. Dividing by $\\sqrt{d/h}$ before applying the softmax function. Recall that $d$ is the feature dimension and $h$ is the number of heads.\n",
        "3. Adding a linear transformation to the output of the attention operation.\n",
        "\n",
        "Only one or two sentences per choice is necessary, but be sure to be specific in addressing what would have happened without each given implementation detail, why such a situation would be suboptimal, and how the proposed implementation improves the situation.\n",
        "\n",
        "**Your Answer:**\n",
        "### START ANSWER HERE ###\n",
        "### END ANSWER HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkfRjeETn2TU"
      },
      "source": [
        "# Transformer Decoder Block\n",
        "\n",
        "The Transformer decoder layer consists of three main components: (1) a self-attention module that processes the input sequence of vectors, (2) a cross-attention module that incorporates additional context (e.g., image features in our case), and (3) a feedforward module that independently processes each vector in the sequence. Complete the implementation of `TransformerDecoderLayer` in `xcs231n/transformer_layers.py` and test it below. The relative error should be less than 1e-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHyiLXjwpU4q",
        "outputId": "c696dc7d-63b5-448e-c3db-8a2dabe4e51a"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N, T, TM, D = 1, 4, 5, 12\n",
        "\n",
        "decoder_layer = TransformerDecoderLayer(D, 2, 4*D)\n",
        "tgt = torch.randn(N, T, D)\n",
        "memory = torch.randn(N, TM, D)\n",
        "tgt_mask = torch.randn(T, T) < 0.5\n",
        "\n",
        "output = decoder_layer(tgt, memory, tgt_mask)\n",
        "\n",
        "\n",
        "expected_output = np.asarray(\n",
        "[[[ 0.7973989,  0.1705188,  0.5952847, -0.3850437,  0.6473090,\n",
        "          -1.7973099, -0.0188924, -1.8536081, -0.8088982,  0.5204701,\n",
        "           1.5894681,  0.5433026],\n",
        "         [-0.8064482,  0.3082525,  0.0128748, -0.1249493,  0.8575743,\n",
        "          -0.8817163,  0.4579798, -1.3449310,  2.2701468, -0.9864848,\n",
        "           0.9998496, -0.7621486],\n",
        "         [-1.1444300,  0.7568803,  0.2958311, -0.1750616,  1.7544298,\n",
        "          -0.8148872,  0.8621378, -0.7340645,  1.1998730, -1.7980157,\n",
        "           0.2740493, -0.4767423],\n",
        "         [-0.6067719,  0.3904864, -0.2153541, -0.8682908,  0.1589641,\n",
        "           0.1644578,  0.4389339, -1.3275373,  1.2310632,  1.2271752,\n",
        "           1.3578585, -1.9509850]]]\n",
        ")\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxBcIdRT7vvz"
      },
      "source": [
        "# Transformer for Image Captioning\n",
        "Now that you have implemented the previous layers, you can combine them to build a Transformer-based image captioning model. Open the file `xcs231n/classifiers/transformer.py` and look at the `CaptioningTransformer` class.\n",
        "\n",
        "Implement the `forward` function of the class. After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-5` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Vxnysk72q6",
        "outputId": "881d4737-0953-46e8-c60f-89f65192f0a6"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N, D, W = 4, 20, 30\n",
        "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
        "V = len(word_to_idx)\n",
        "T = 3\n",
        "\n",
        "transformer = CaptioningTransformer(\n",
        "    word_to_idx,\n",
        "    input_dim=D,\n",
        "    wordvec_dim=W,\n",
        "    num_heads=2,\n",
        "    num_layers=2,\n",
        "    max_length=30\n",
        ")\n",
        "\n",
        "features = torch.randn(N, D)\n",
        "captions = torch.randint(0, V, (N, T))\n",
        "\n",
        "scores = transformer(features, captions)\n",
        "\n",
        "expected_scores = np.asarray(\n",
        "[[[ 0.75886, -0.69655, -0.95691],\n",
        "         [ 0.64593, -0.44269, -1.61055],\n",
        "         [ 0.78522, -0.91518, -0.43513]],\n",
        "\n",
        "        [[ 0.50102, -0.51465, -0.79957],\n",
        "         [ 0.55627, -0.79746, -1.10802],\n",
        "         [ 0.61483, -0.79027, -1.22522]],\n",
        "\n",
        "        [[ 0.54926, -0.53859, -1.67718],\n",
        "         [ 0.63181, -0.35069, -1.16072],\n",
        "         [ 0.13742, -1.03246, -1.28366]],\n",
        "\n",
        "        [[ 0.81889, -0.40008, -0.81413],\n",
        "         [ 0.71647, -0.97332, -1.54536],\n",
        "         [ 0.29304, -0.54018, -1.08682]]] \n",
        ")\n",
        "\n",
        "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwUhSxEx7qyv"
      },
      "source": [
        "# Overfit Transformer Captioning Model on Small Data\n",
        "Run the following to overfit the Transformer-based captioning model on the same small dataset as we used for the RNN previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-HMqJq4T7qyv",
        "outputId": "689d27b6-1203-461d-cdad-48e3442d4a89"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "data = load_coco_data(max_train=50)\n",
        "\n",
        "transformer = CaptioningTransformer(\n",
        "          word_to_idx=data['word_to_idx'],\n",
        "          input_dim=data['train_features'].shape[1],\n",
        "          wordvec_dim=256,\n",
        "          num_heads=2,\n",
        "          num_layers=2,\n",
        "          max_length=30\n",
        "        )\n",
        "\n",
        "\n",
        "transformer_solver = CaptioningSolverTransformer(transformer, data, idx_to_word=data['idx_to_word'],\n",
        "           num_epochs=100,\n",
        "           batch_size=25,\n",
        "           learning_rate=0.001,\n",
        "           verbose=True, print_every=10,\n",
        "         )\n",
        "\n",
        "transformer_solver.train()\n",
        "\n",
        "# Plot the training losses.\n",
        "plt.plot(transformer_solver.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcRNyolJ7qyw"
      },
      "source": [
        "Print final training loss. You should see a final loss of less than 0.05 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPU95Nv27qyx",
        "outputId": "3d74b725-2ae5-4710-c370-7fc9bcdb0308",
        "tags": [],
        "test": "transformer_final_training_loss"
      },
      "outputs": [],
      "source": [
        "print('Final loss: ', transformer_solver.loss_history[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R-SUFxf7qyx"
      },
      "source": [
        "# Transformer Sampling at Test Time\n",
        "The sampling code has been written for you. You can simply run the following to compare with the previous results with the RNN. As before the training results should be much better than the validation set results, given how little data we trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K4uQMkIC7qyy",
        "outputId": "b3e5ed37-a14e-4e0b-a64e-4135336a821e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# If you get an error, the URL just no longer exists, so don't worry!\n",
        "# You can re-sample as many times as you want.\n",
        "for split in ['train', 'val']:\n",
        "    minibatch = sample_coco_minibatch(data, split=split, batch_size=2)\n",
        "    gt_captions, features, urls = minibatch\n",
        "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
        "\n",
        "    sample_captions = transformer.sample(features, max_length=30)\n",
        "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
        "\n",
        "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
        "        img = image_from_url(url)\n",
        "        # Skip missing URLs.\n",
        "        if img is None: continue\n",
        "        plt.imshow(img)\n",
        "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZMSyzMWyQpK"
      },
      "source": [
        "# Vision Transformer (ViT)\n",
        "\n",
        "[Dosovitskiy et. al.](https://arxiv.org/abs/2010.11929) showed that applying a transformer model on a sequence of image patches (referred to as Vision Transformer) not only achieves impressive performance but also scales more effectively than convolutional neural networks when trained on large datasets. We will build a version of Vision Transformer using our existing implementation of transformer components and train it on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAntTB4F0yHC"
      },
      "source": [
        "Vision Transformer converts input image into a sequence of patches of fixed size and embed each patch into a latent vector. In `xcs231n/transformer_layers.py`, complete the implementation of `PatchEmbedding` and test it below. You should see relative error less than 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRiRu3pN7qyz",
        "outputId": "9b0e8ee6-37c4-472f-a55c-f9028880585a"
      },
      "outputs": [],
      "source": [
        "from xcs231n.transformer_layers import PatchEmbedding\n",
        "\n",
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N = 2\n",
        "HW = 16\n",
        "PS = 8\n",
        "D = 8\n",
        "\n",
        "patch_embedding = PatchEmbedding(\n",
        "    img_size=HW,\n",
        "    patch_size=PS,\n",
        "    embed_dim=D\n",
        ")\n",
        "\n",
        "x = torch.randn(N, 3, HW, HW)\n",
        "output = patch_embedding(x)\n",
        "\n",
        "expected_output = np.asarray([\n",
        "        [[-0.6312704 ,  0.02531429,  0.6112642 , -0.49089882,\n",
        "          0.01412961, -0.6959372 , -0.32862484, -0.45402682],\n",
        "        [ 0.18816411, -0.08142513, -0.9829535 , -0.23975623,\n",
        "         -0.23109074,  0.97950286, -0.40997326,  0.7457837 ],\n",
        "        [ 0.01810865,  0.15780598, -0.91804236,  0.36185235,\n",
        "          0.8379501 ,  1.0191797 , -0.29667392,  0.20322265],\n",
        "        [-0.18697818, -0.45137224, -0.40339014, -1.4381214 ,\n",
        "         -0.43450755,  0.7651071 , -0.83683825, -0.16360264]],\n",
        "\n",
        "       [[-0.39786366,  0.16201034, -0.19008337, -1.0602452 ,\n",
        "         -0.28693503,  0.09791763,  0.26614824,  0.41781986],\n",
        "        [ 0.35146567, -0.4469593 , -0.1841726 ,  0.45757473,\n",
        "         -0.61304873, -0.29104248, -0.16124889, -0.14987172],\n",
        "        [-0.2996967 ,  0.27353522, -0.09929767,  0.01973832,\n",
        "         -1.2312065 , -0.6374332 , -0.22963578,  0.55696607],\n",
        "        [-0.93818814,  0.02465284, -0.21117875,  1.1860403 ,\n",
        "         -0.06137538, -0.21062079, -0.094347  ,  0.50032747]]])\n",
        "\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-equGLTX7rxV"
      },
      "source": [
        "The sequence of patch vectors is processed by transformer encoder layers, each consisting of a self-attention and a feed-forward module. Since all vectors attend to one another, attention masking is not strictly necessary. However, we still implement it for the sake of consistency.\n",
        "\n",
        "Implement `TransformerEncoderLayer` in `xcs231n/transformer_layers.py` and test it below. You should see relative error less than 1e-6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YIr_Fxv5xvy",
        "outputId": "15552201-94de-454e-f60f-41dbfbcef6a3"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "from xcs231n.transformer_layers import TransformerEncoderLayer\n",
        "\n",
        "N, T, TM, D = 1, 4, 5, 12\n",
        "\n",
        "encoder_layer = TransformerEncoderLayer(D, 2, 4*D)\n",
        "x = torch.randn(N, T, D)\n",
        "x_mask = torch.randn(T, T) < 0.5\n",
        "\n",
        "output = encoder_layer(x, x_mask)\n",
        "\n",
        "expected_output = np.asarray(\n",
        "[[[-0.468097, -0.209465,  0.574843, -1.007282,  1.386149,  0.235459,\n",
        "           0.942292, -0.572514, -1.704719, -1.222015,  1.617727,  0.427621],\n",
        "         [-0.162546,  1.103722, -1.028051, -0.307341, -2.160513,  1.275178,\n",
        "          -0.022269, -0.924952,  1.417923, -0.112859,  0.406233,  0.515477],\n",
        "         [ 0.255601,  1.003893, -1.254898,  0.917847, -1.967772, -0.233367,\n",
        "          -0.524232, -0.234828,  0.190839,  2.015347,  0.181081, -0.349513],\n",
        "         [-0.765535,  0.600895, -0.538682, -1.519948,  1.226579,  0.481148,\n",
        "           0.707173,  0.555546, -1.374603, -1.377802,  0.768860,  1.236369]]]\n",
        ")\n",
        "\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1msDyvLseyI"
      },
      "source": [
        "Take a look at the `VisionTransformer` implementation in `xcs231n/classifiers/transformer.py`.\n",
        "\n",
        "For classification, ViT divides the input image into patches and processes the sequence of patch vectors using a transformer. Finally, all the patch vectors are average-pooled and used to predict the image class. We will use the same 1D sinusoidal positional encoding to inject ordering information, though 2D sinusoidal and learned positional encodings are also valid choices.\n",
        "\n",
        "Complete the ViT forward pass and test it below. You should see relative error less than 1e-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHZ1LMuRpOiV",
        "outputId": "b10e79cb-cc16-46d1-8582-0c490acd9dd1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "from xcs231n.classifiers.transformer import VisionTransformer\n",
        "\n",
        "imgs = torch.randn(3, 3, 32, 32)\n",
        "transformer = VisionTransformer()\n",
        "scores = transformer(imgs)\n",
        "expected_scores = np.asarray(\n",
        "[[-0.158277,  0.122878, -0.026341, -0.180966, -0.071095, -0.064299,\n",
        "          0.148130,  0.035577,  0.200194,  0.099250],\n",
        "        [-0.097436,  0.181270, -0.060364, -0.089896, -0.105121, -0.092695,\n",
        "          0.162580,  0.045888,  0.147600,  0.097863],\n",
        "        [-0.087285,  0.207854, -0.039861, -0.109968, -0.133690, -0.103884,\n",
        "          0.145094,  0.052029,  0.148208,  0.103894]] \n",
        ")\n",
        "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Redma2A5BZA"
      },
      "source": [
        "\n",
        "We will first verify our implementation by overfitting it on one training batch. Tune learning rate and weight decay accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myf9A3UK4TpM",
        "outputId": "80441093-0ea2-401b-ada3-c1cb2c8bbcd2"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU2pQigB9kxp",
        "outputId": "ea03f5ed-ee7e-4b61-94c6-dd5ab852afc8"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4  # Experiment with this\n",
        "weight_decay = 1.e-4  # Experiment with this\n",
        "\n",
        "# ### START CODE HERE ###\n",
        "# ### END CODE HERE ###\n",
        "\n",
        "batch = next(iter(DataLoader(train_data, batch_size=64, shuffle=False)))\n",
        "model = VisionTransformer(dropout=0.0)\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "model.train()\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    imgs, target = batch\n",
        "    out = model(imgs)\n",
        "    loss = loss_criterion(out, target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    top1 = (out.argmax(-1) == target).float().mean().item()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"[{epoch}/{epochs}] Loss {loss.item():.6f}, Top-1 Accuracy: {top1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mrgWcfEE8XE",
        "outputId": "43961537-62d6-41fc-b0e9-3c5695a41eb0",
        "test": "vit_overfit_accuracy"
      },
      "outputs": [],
      "source": [
        "# You should get perfect 1.00 accuracy\n",
        "print(f\"Overfitting ViT on one batch. Top-1 accuracy: {top1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5-AVJNGYS9"
      },
      "source": [
        "Now we will train it on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "c5ce76ad617c4e7a9b8563b0665b673c",
            "a652dbe89a4e42eda7e8012a78456006",
            "600e386d2bf44c61a91de90b6873763c",
            "705f1c23d33a464da83ff2af0841246f",
            "7a7613c8bcc44b21baba4418ba0c0311",
            "11b799c825e545d1b0beb63edefedc1f",
            "b178c15dd8f54724857a0d456f16c0e2",
            "82db9af319e342ffaf955b1a3097d724",
            "8618185c0f8347339c44eaa8d9b11ca5",
            "71c4bc0fbdaf4b6ca015021ea471ec3d",
            "f1fb2d05d3af4287b60f27960f181ebd",
            "0d8b7617e0e4417494826e1a4872e2dd",
            "ebc6179df8d44ea7b79c93ecbd461785",
            "311cfdea71d8454a80ad68fc78ca61fa",
            "cc12955635624a6e9acd8ee6e2476980",
            "af3510dc25f14298b89edc925afe99da",
            "4d47a40f6c264fb4a991e388c22a7d00",
            "79a01f9d270f414a976e5bfe3b8a0fc7",
            "a3e4822eaefb4f09830d27f99ad3df2e",
            "af388e9f7b634d818abd988f939485cc",
            "63c06358e0ea4d8ca3625c5c8b899a32",
            "44928a4f060741e49173f0075d69bbc5",
            "62365b136d2348f99c5ab5a00dad8462",
            "c6e2c0bc4bcc4aa9a7ed53e712c8273e",
            "7480f391078c44648f99fb5500a05f90",
            "d0d1c9d80ac5459390714cd75bdebb88",
            "92fa2597bb2e47aba233e92a3cbd7974",
            "653616ae55214a169ff7413f3b5a7b5e",
            "6b091249581448b682c61e2cd1c630a2",
            "167c60d31f2045039322af7f76055697",
            "e9270215291e43c98af019175fffdf45",
            "33d1b506bdee413ba4528020cf513351",
            "ad16267f3756454baadfc15dd0fe3dcd",
            "4243658caaff4eb7bab8f0a8822293a7",
            "3582aa676163469cbdab43729e2aaf58",
            "8eaa124ca2fd4496b914e4bb93ccd757",
            "9f9f8745290947f093f2df298d504c19",
            "c11e6e68cb5c40ad88a6f6719de76864",
            "aaa5c276772a44cb9776aa07af036d28",
            "a8b03e68f2e84be9b81226916390a5b6",
            "5167f9aa07bd4c3da3c6b80cea41131c",
            "77a5a1ee775f48eea1a93cec8da52f14",
            "77b2349aca7846eb95c3fd209d8244ed",
            "92bd9d6a9975499da6aee8a33fb41e2d"
          ]
        },
        "id": "9W2dUKrz8MbG",
        "outputId": "3f806c65-a330-4d58-8b67-52cc106f024d"
      },
      "outputs": [],
      "source": [
        "from xcs231n.classification_solver_vit import ClassificationSolverViT\n",
        "\n",
        "############################################################################\n",
        "# TODO: Train a Vision Transformer model that achieves over 0.45 test      #\n",
        "# accuracy on CIFAR-10 after 2 epochs by adjusting the model architecture  #\n",
        "# and/or training parameters as needed.                                    #\n",
        "#                                                                          #\n",
        "# Note: If you want to use a GPU runtime, go to `Runtime > Change runtime  #\n",
        "# type` and set `Hardware accelerator` to `GPU`. This will reset Colab,    #\n",
        "# so make sure to rerun the entire notebook from the beginning afterward.  #\n",
        "############################################################################\n",
        "\n",
        "\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 0.0\n",
        "batch_size = 64\n",
        "model = VisionTransformer()  # You may want to change the default params.\n",
        "\n",
        "\n",
        "# ### START CODE HERE ###\n",
        "# ### END CODE HERE ###\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n",
        "\n",
        "solver = ClassificationSolverViT(\n",
        "    train_data=train_data,\n",
        "    test_data=test_data,\n",
        "    model=model,\n",
        "    num_epochs = 2,  # Don't change this\n",
        "    learning_rate = learning_rate,\n",
        "    weight_decay = weight_decay,\n",
        "    batch_size = batch_size,\n",
        ")\n",
        "\n",
        "solver.train('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1xe22cuGsnw",
        "outputId": "77b74009-1c53-46bd-98ba-5ff3ad03396c",
        "test": "vit_test_accuracy"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy on test set: {solver.results['best_test_acc']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtjQqsDBm9or"
      },
      "source": [
        "# Inline Question 2\n",
        "\n",
        "Despite their recent success in large-scale image recognition tasks, ViTs often lag behind traditional CNNs when trained on smaller datasets. What underlying factor contribute to this performance gap? What techniques can be used to improve the performance of ViTs on small datasets?\n",
        "\n",
        "**Your Answer**: \n",
        "\n",
        "### START ANSWER HERE ###\n",
        "### END ANSWER HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cau-UmPBJ3ez"
      },
      "source": [
        "# Inline Question 3\n",
        "\n",
        "How does the computational cost of the self-attention layers in a ViT change if we independently make the following changes?\n",
        "\n",
        "(i) Double the hidden dimension.\n",
        "(ii) Double the height and width of the input image.\n",
        "(iii) Double the patch size.\n",
        "(iv) Double the number of layers.\n",
        "\n",
        "**Your Answer**: \n",
        "\n",
        "### START ANSWER HERE ###\n",
        "### END ANSWER HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Submission\n",
        "\n",
        "**You will need to submit the answers from this notebook on Gradescope.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
